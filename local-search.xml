<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>推荐系统实践（项亮）第四章</title>
    <link href="/2022/04/07/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E5%9B%9B%E7%AB%A0/"/>
    <url>/2022/04/07/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E5%9B%9B%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="第四章-利用用户标签数据"><a href="#第四章-利用用户标签数据" class="headerlink" title="第四章 利用用户标签数据"></a>第四章 利用用户标签数据</h1><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407213140546.png" alt=" " style="zoom: 67%;"/></div><p>推荐系统的目的是联系用户的兴趣和物品，这种联系需要不同的媒介。除了之前的$ItemCF$和$UserCF$,还有一种重要的方式是<strong>通过一些特征</strong>联系用户和物品,给用户推荐那些具有用户喜欢的特征的物品.这里的特征有不同的表现方式，比如可以表现为物品的属性集合（e.g., 对于图书，属性集合包括作者、出版社、主题和关键词等），也可以表现为隐语义向量（latent factor vector），这可以通过前面提出的隐语义模型学习得到。本章将讨论一种重要的<strong>特征表现方式</strong>——标签。</p><p>标签是一种无层次化结构的、用来描述信息的关键词，它可以用来描述物品的语义。根据给物品打标签的人不同，标签应用一般分为两种：一种是让作者或者专家给物品打标签；另一种是让普通用户给物品打标签，也就是UGC（User Generated Content，用户生成的内容）的标签应用。UGC的标签系统是一种表示用户兴趣和物品语义的重要方式。当一个用户对一个物品打上一个标签，这个标签一方面<strong>描述了用户的兴趣</strong>，另一方面则<strong>表示了物品的语义</strong>，从而将用户和物品联系了起来。因此本章主要讨论UGC的标签应用，研究用户给物品打标签的行为，探讨如何通过分析这种行为给用户进行个性化推荐。</p><h1 id="4-1-UGC标签系统的代表应用"><a href="#4-1-UGC标签系统的代表应用" class="headerlink" title="4.1 UGC标签系统的代表应用"></a>4.1 UGC标签系统的代表应用</h1><p>UGC标签系统的鼻祖Delicious、论文书签网站CiteULike、音乐网站Last.fm、视频网站Hulu、书和电影评论网站豆瓣等等，不做过多阐述</p><h1 id="4-2-标签系统中的推荐问题"><a href="#4-2-标签系统中的推荐问题" class="headerlink" title="4.2 标签系统中的推荐问题"></a>4.2 标签系统中的推荐问题</h1><p>标签系统中的推荐问题主要有以下两个。</p><ul><li>如何利用用户打标签的行为为其推荐物品（基于标签的推荐）？</li><li>如何在用户给物品打标签时为其推荐适合该物品的标签（标签推荐）？</li></ul><p>为了研究上面的两个问题，我们首先需要解答下面3个问题。</p><ul><li>用户为什么要打标签？</li><li>用户怎么打标签？</li><li>用户打什么样的标签？</li></ul><h2 id="4-2-1-用户为什么进行标注"><a href="#4-2-1-用户为什么进行标注" class="headerlink" title="4.2.1 用户为什么进行标注"></a>4.2.1 用户为什么进行标注</h2><p>社会维度</p><ul><li>有些用户标注是给内容上传者使用的（便于上传者组织自己的信息）</li><li>有些用户标注是给广大用户使用的（便于帮助其他用户找到信息）</li></ul><p>功能维度</p><ul><li>有些标注用于更好地组织内容，方便用户将来的查找</li><li>而另一些标注用于传达某种信息，比如照片的拍摄时间和地点等。</li></ul><h2 id="4-2-2-用户如何打标签"><a href="#4-2-2-用户如何打标签" class="headerlink" title="4.2.2 用户如何打标签"></a>4.2.2 用户如何打标签</h2><p>使用的另一个数据集发现效果不好，就用这个<a href="https://grouplens.org/datasets/hetrec-2011/">数据集：delicious-2k</a></p><p>首先通过研究数据集总结用户标注行为的一些统计规律。我们定义：一个标签被一个用户使用在一个物品上，它的流行度就加一。可以看出标签流行度符合长尾分布，</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220410205647970.png" alt="image-20220410205647970" style="zoom:67%;" /></div><p><a href="https://github.com/hiccgoal/--/blob/main/4.2.2%E7%94%A8%E6%88%B7%E5%A6%82%E4%BD%95%E6%89%93%E6%A0%87%E7%AD%BE.ipynb">实验代码</a></p><h2 id="4-2-3-用户打什么样的标签"><a href="#4-2-3-用户打什么样的标签" class="headerlink" title="4.2.3  用户打什么样的标签"></a>4.2.3  用户打什么样的标签</h2><p>在用户看到一个物品时，打的标签可能是很多样的，并不会按我们的想法操作。很多网站也设计了自己的标签分类系统。有人将Delicious数据集的标签分为如下几类：</p><ul><li>表明物品是什么</li><li>表明物品的种类——article；blog；book</li><li>表明谁拥有物品——作者</li><li>表达用户的观点——funny；boring</li><li>用户相关的标签——my favorite；my comment</li><li>用户的任务——to read；job search</li></ul><h2 id="4-3-基于标签的推荐系统"><a href="#4-3-基于标签的推荐系统" class="headerlink" title="4.3 基于标签的推荐系统"></a>4.3 基于标签的推荐系统</h2><p><a href="https://github.com/hiccgoal/RecommendSystemPractice/blob/master/Chapter4/%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E6%A0%87%E7%AD%BE%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.ipynb">参考代码</a></p><p>一个用户标签行为的数据集一般由一个三元组的集合表示，其中$(u,i,b)$代表用户$u$ 给商品$i$打上标签$b$。（真实标签行为数据还包括用户打标签的时间、用户的属性数据、物品的属性数据等等）,为了集中讨论标签数据，之后的实验只考虑上面定义的三元组形式的数据，即用户的每一次打标签行为都用一个三元组（用户、物品、标签）表示。</p><h2 id="4-3-1-实验设置"><a href="#4-3-1-实验设置" class="headerlink" title="4.3.1 实验设置"></a>4.3.1 实验设置</h2><p>首先把数据集分割（分割的键值是用户和物品，防止出现用户对<strong>一个物品</strong>打的多个标签被分到训练集和测试集中）。令$R(u)$为给用户$u$的长度为$N$的推荐列表,里面包含我们认为用户会打标签的物品。$T(u)$是测试集中用户$u$实际上打过标签的物品集合。<br>$$<br>Precision&#x3D;\frac{|R(u)\cap T(u)|}{|R(u)|}\<br>Recall&#x3D;\frac{|R(u)\cap T(u)|}{|T(u)|}<br>$$<br>同时评测覆盖率：<br>$$<br>Coverage&#x3D;\frac{\cup _{u\in U}R(u)}{|I|}<br>$$<br>第一章提到，多样性的定义取决于相似度的定义。这里使用物品标签的余弦相似度度量物品间的相似度。对于物品$i$，<code>item_tags[i]</code>存储了物品<code>i</code>的标签向量，其中<code>item_tags[i][b]</code>是对物品$i$打标签$b$的次数。</p><p>得到物品相似度后，通过下式计算<strong>一个推荐列表</strong>的多样性：<br>$$<br>Diversity&#x3D;1-\frac{\sum_{i\in R(u)}\sum_{j\in R(u,)j\neq i}Sim(item_tags[i],item_tags[j])}{\dbinom {|R(u)|}2}<br>$$<br>计算所有用户推荐列表多样性的平均值即为推荐系统的多样性。</p><p>推荐结果的新颖性用推荐结果的平均热门程度（AveragePopularity）度量，对于物品$i$，定义它的流行度<code>item_pop(i)</code>为给这个物品打过标签的<strong>用户数</strong>，定义推荐系统的平均热门度如下：<br>$$<br>AveragePopularity&#x3D;\frac{\sum_{u}\sum_{i\in R(u)}log(1+item_pop(i))}{\sum_{u}\sum_{i\in R(u)}1}<br>$$</p><h3 id="4-3-2-一个简单的算法"><a href="#4-3-2-一个简单的算法" class="headerlink" title="4.3.2 一个简单的算法"></a>4.3.2 一个简单的算法</h3><ul><li>统计每个用户最常用的标签</li><li>对每个标签，统计被打过这个标签次数最多的物品</li><li>对一个用户，首先找到他常用的标签，然后找到具有这些标签的最热门物品推荐给这个用户</li></ul><p>则用户$u$对物品$i$的兴趣可表示为：<br>$$<br>p(u,i)&#x3D;\sum_bn_{u,b}n_{b,i}<br>$$<br>$B(u)$是用户$u$打过的标签集合，$B(i)$是物品$i$被打过的标签集合，$n_{u,b}$是用户$u$打过标签$b$的次数，$n_{b,i}$是物品$i$被打过标签$b$的次数。在代码中，<code>user_tags[u][b]</code> &#x3D; $n_{u,b}$ ；<code>tag_items[b][i]</code> &#x3D; $n_{b,i}$</p><p>之后，便可根据兴趣排名进行推荐</p><h3 id="4-3-3-算法改进"><a href="#4-3-3-算法改进" class="headerlink" title="4.3.3 算法改进"></a>4.3.3 算法改进</h3><h4 id="1-TFIDF"><a href="#1-TFIDF" class="headerlink" title="1 TFIDF"></a>1 TFIDF</h4><h5 id="TagBasedTFIDF"><a href="#TagBasedTFIDF" class="headerlink" title="TagBasedTFIDF"></a>TagBasedTFIDF</h5><p>老生常谈的问题：上面的算法利用用户的标签向量对用户兴趣建模，其中每个标签都是用户使用过的标签，而标签的权重是用户使用该标签的次数，这会造成给热门标签对应的热门物品很大的权重。例如一名用户使用了10次标签$b_1$：Robdingnagian（巨大的）和1次标签$b_2$：Interesting，即$10&#x3D;n_{u,b_1}&gt;n_{u,b_2}&#x3D;1$。显然前者是一个冷门标签，后者是热门标签，而且该用户更喜欢巨大的而非有趣的。但对大多数商品来说，其标签是$b_1$的概率远远小于$b_2$,即$n_{b_1,i}&lt;&lt;n_{b_2,i}$,因此按照上面公式最终得到的推荐商品可能都是有趣的而非巨大的，不符合该用户的口味。</p><p>借鉴TF-IDF的思想进行改进，对<strong>热门标签</strong>进行惩罚：<br>$$<br>p(u,i)&#x3D;\sum_b\frac{n_{u,b}}{log(1+n_b^{(u)})}n_{b,i}<br>$$<br>$n_b^{(u)}$记录了<strong>标签$b$<strong>被多少个</strong>不同</strong>的用户<strong>使用过</strong>。</p><h5 id="TagBasedTFIDF-1"><a href="#TagBasedTFIDF-1" class="headerlink" title="TagBasedTFIDF++"></a>TagBasedTFIDF++</h5><p>同样也可以对<strong>热门物品</strong>进行惩罚：<br>$$<br>p(u,i)&#x3D;\sum_b\frac{n_{u,b}}{log(1+n_b^{(u)})}\frac{n_{b,i}}{log(1+n_i^{(u)})}<br>$$<br>$n_i^{(u)}$记录了<strong>物品$i$<strong>被多少个</strong>不同</strong>的用户<strong>打过标签</strong>。</p><h4 id="2-数据稀疏性"><a href="#2-数据稀疏性" class="headerlink" title="2 数据稀疏性"></a>2 数据稀疏性</h4><p>在前面的算法中，用户兴趣和物品的联系是通过$B(u)\cap B(i)$中的标签建立的，但对于新物品或新用户，这个值就会很小，因此需要对标签集合做扩展。例如某用户使用一个标签，就可以把这个标签的相似标签也加入进来。标签扩展有很多方法，如话题模型（topic model），下面介绍一种简单的基于邻域的方法：</p><p>标签扩展可理解为计算标签间的相似度。最简单的情况是，如果有同义词词典，就可以根据这个词典进行扩展；也可以从数据中统计出标签的相似度。即当两个标签同时出现在很多物品的标签集合中时，就可以认为这两个标签具有较大的相似度。<br>$$<br>sim(b,b’)&#x3D;\frac{\sum_{i\in N(b)\cap N(b’)}n_{b,i}n_{b’,i}}{\sqrt {\sum_{i\in N(b)}n_{b,i}^2}\sqrt{  \sum_{i\in N(b’)} n_{b’,i}^2}}<br>$$<br>$N(b)$为具有标签$b$的<strong>物品集合</strong>，$n_{b,i}$是物品$i$被打过标签$b$的次数。</p><h4 id="3-标签清理"><a href="#3-标签清理" class="headerlink" title="3 标签清理"></a>3 标签清理</h4><p>标签清理的原因：</p><ul><li>不是所有标签都能反应用户的兴趣，例如用户给某视频打了“not funny”标签,不能认为他对该标签感兴趣，并且给其他用户推荐具有该标签的其他视频；但如果对视频打了“成龙”标签，就可以认为他对成龙主演的视频有兴趣，从而推荐。</li><li>另外，标签系统经常出现词形不同、词义相同的标签</li><li>将标签作为推荐解释（如果要把标签呈现给用户，就需要高质量的标签，不能包含无意义的停止词，意义相同的词等）</li></ul><p>一般有以下标签清理方法：</p><ul><li>去除词频很高的词</li><li>去除因词根不同造成的同义词，比如recommender system和recommendation system</li><li>去除因分隔符造成的同义词，比如collaborative_filtering和collaborative-filtering</li></ul><p>当然，为了控制标签的质量，也可以让用户进行反馈</p><h3 id="4-3-4-基于图的推荐算法"><a href="#4-3-4-基于图的推荐算法" class="headerlink" title="4.3.4 基于图的推荐算法"></a>4.3.4 基于图的推荐算法</h3><p>前面的算法不够系统化和理论化（但可能很work），下面为利用图模型做基于标签数据的个性化推荐。</p><p>首先定义三种不同的顶点：用户顶点$v(u)$；物品顶点$v(i)$； 标签顶点$v(b)$。对于每一个行为$(u,i,b)$，就在图中增加三条边（如果两个顶点已经相连，就将边的权重增加1）。图4-11是一个简单的用户-物品-标签图。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220411113036680.png" alt="image-20220411113036680" style="zoom: 40%;" /></div><p>定义出用户-物品-标签图后，就可以利用<a href="http://hiccgoal.space/2022/03/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%BA%8C%E7%AB%A0/#2-6-2-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95">第二章的PersonalRank算法</a>计算所有物品节点相对于当前用户节点在图上的相关性，然后按照相关性排序、推荐。</p><h4 id="用图模型解释前面的简单算法（SimpleTagGraph）"><a href="#用图模型解释前面的简单算法（SimpleTagGraph）" class="headerlink" title="用图模型解释前面的简单算法（SimpleTagGraph）"></a>用图模型解释前面的简单算法（SimpleTagGraph）</h4><p>简单算法中的$p(u,i)&#x3D;\sum_bn_{u,b}n_{b,i}$可以看作$p(i|u)&#x3D;\sum_bp{(b|u)}p{(i|b)}$，它假设用户对物品的兴趣通过标签传递，因此该公式可用更简单的图来建模（SimpleTagGraph），对一个用户行为只增加两条边（少了用户和物品节点之间的边）。</p><p>如下例，构建SimpleTagGraph后，利用PersonalRank算法,就等价于前面提出的简单推荐算法。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220411114252732.png" alt="image-20220411114252732" style="zoom:67%;" /></div><h3 id="4-3-5-基于标签的推荐解释"><a href="#4-3-5-基于标签的推荐解释" class="headerlink" title="4.3.5 基于标签的推荐解释"></a>4.3.5 基于标签的推荐解释</h3><p>要让用户直观上感觉推荐结果有道理是很困难的，豆瓣将推荐结果的可解释性拆分成了两部分，<strong>首先</strong>让用户觉得标签云是有道理的，<strong>然后</strong>让用户觉得从某个标签推荐出某本书也是有道理的。因为生成让用户觉得有道理的标签云比生成让用户觉得有道理的推荐图书更加简单，标签和书的关系就更容易让用户觉得有道理，从而让用户最终觉得推荐出来的书也是很有道理的。</p><p>豆瓣这样组织推荐结果页面有很多好处，首先是<strong>提高了推荐结果的多样性</strong>：一个用户的兴趣在长时间内是很广泛的，但在某一天却比较具体。因此，我们如果想在某一天击中用户当天的兴趣，是非常困难的。通过标签云，展示了用户的所有兴趣，然后让用户自己根据他今天的兴趣选择相关的标签，得到推荐结果，从而极大地提高了推荐结果的多样性，使得推荐结果更容易满足用户多样的兴趣。<br>同时，标签云也提供了<strong>推荐解释功能</strong>：用户通过这个界面可以知道豆瓣给自己推荐的每一本书都是基于它认为自己对某个标签感兴趣。而对于每个标签，用户总能通过回忆自己之前的行为知道自己是否真的对这个标签感兴趣。</p><h2 id="4-4-给用户推荐标签"><a href="#4-4-给用户推荐标签" class="headerlink" title="4.4 给用户推荐标签"></a>4.4 给用户推荐标签</h2><h3 id="4-4-1-为什么要给用户推荐标签"><a href="#4-4-1-为什么要给用户推荐标签" class="headerlink" title="4.4.1 为什么要给用户推荐标签"></a>4.4.1 为什么要给用户推荐标签</h3><ul><li>方便用户输入标签 （提高参与度）</li><li>提高标签质量（保证词表不出现太多的同义词，出现的都是一些比较热门的、有代表性的词）</li></ul><h3 id="4-4-2-如何给用户推荐标签"><a href="#4-4-2-如何给用户推荐标签" class="headerlink" title="4.4.2 如何给用户推荐标签"></a>4.4.2 如何给用户推荐标签</h3><p><a href="https://github.com/hiccgoal/RecommendSystemPractice/blob/master/Chapter4/%E7%BB%99%E7%94%A8%E6%88%B7%E6%8E%A8%E8%8D%90%E6%A0%87%E7%AD%BE.ipynb">参考代码</a></p><p>第0种：给用户$u$推荐整个系统里最热门的标签（PopularTags）</p><p>第1种：给用户$u$推荐物品$i$上最热门的标签 （ItemPopularTags）</p><p>第2种：给用户$u$推荐他自己经常使用的标签 （UserPopularTags）</p><p>第3种：前两种方法的融合，通过一个系数将上面的推荐结果加权（HybridPopularTags）</p><p>这类基于用户常用标签和物品常用标签的算法对新用户或不热门的物品很难有推荐结果，一般有两个解决思路：</p><ul><li>从物品的内容数据中抽取关键词作为标签，在上下文广告领域这类研究很多</li><li>针对有结果但不多的情况，可使用关键词扩展</li></ul><h3 id="4-4-4-基于图的标签推荐算法"><a href="#4-4-4-基于图的标签推荐算法" class="headerlink" title="4.4.4 基于图的标签推荐算法"></a>4.4.4 基于图的标签推荐算法</h3><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220411113036680.png" alt="image-20220411113036680" style="zoom: 40%;" /></div><p>同样是这个图，此时的问题是用户遇到物品时，会给物品打什么样的标签。因此重新定义顶点的启动概率，如下所示：<br>$$<br>r_{v(k)}&#x3D;\left{<br>\begin{aligned}<br>\alpha &amp;  &amp; v(k)&#x3D;v(u), \<br>1-\alpha &amp;  &amp; v(k)&#x3D;v(i), \<br>0 &amp;  &amp; 其他<br>\end{aligned}<br>\right.<br>$$<br>即只有用户$u$和物品$i$对应的顶点有非0的启动概率。</p><h2 id="4-5-总结"><a href="#4-5-总结" class="headerlink" title="4.5 总结"></a>4.5 总结</h2><p>主要讨论了UGC标签在推荐系统中的应用。标签作为描述语义的重要媒介，无论是对于描述用户兴趣还是表示物品的内容都有很重要的意义。标签在推荐系统中的应用主要集中在两个问题上，一个是如何利用用户打标签的行为给用户推荐物品，另一个是如何给用户推荐标签。本章在深入分析用户标签行为的基础上对这两个问题进行了深入探讨。</p><p>关于标签的研究有很多新的方法，比如张量分解（tensor factorization）、基于LDA的算法、基于图的算法等。不过这些算法很多具有较高的复杂度，在实际系统中应用起来还有很多实际的困难需要解决。</p>]]></content>
    
    
    
    <tags>
      
      <tag>读书笔记</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推荐系统实践（项亮）第三章</title>
    <link href="/2022/04/07/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%B8%89%E7%AB%A0/"/>
    <url>/2022/04/07/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="第三章-推荐系统冷启动问题"><a href="#第三章-推荐系统冷启动问题" class="headerlink" title="第三章 推荐系统冷启动问题"></a>第三章 推荐系统冷启动问题</h1><p>冷启动问题：如何在没有大量用户数据的情况下设计个性化推荐系统？</p><h2 id="3-1-冷启动问题简介"><a href="#3-1-冷启动问题简介" class="headerlink" title="3.1 冷启动问题简介"></a>3.1 冷启动问题简介</h2><p>冷启动问题（cold start）主要分三类：</p><ul><li><strong>用户冷启动</strong>    对于无行为数据的新用户，如何做个性化推荐</li><li><strong>物品冷启动</strong>    如何将新的物品推荐给可能对它感兴趣的用户</li><li><strong>系统冷启动</strong>    如何在一个新开发的网站（无用户，无用户行为，只有一些物品的信息）上设计个性化推荐系统</li></ul><p>一般有以下解决方案：</p><ul><li><strong>提供非个性化推荐</strong>    e.g.,热门排行榜.收集一定用户数据后,再切换为个性化推荐</li><li>利用用户注册时提供的年龄,性别等数据做粗粒度的个性化</li><li>利用授权的社交账号导入用户的社交网站上的好友信息,然后给用户推荐其好友喜欢的物品</li><li>要求用户在登陆时对一些物品进行反馈,收集用户对这些物品的兴趣信息,然后给用户推荐类似的物品</li><li>对于新加入的物品,可以利用内容信息,将他们推荐给喜欢过类似物品的用户</li><li>在系统冷启动时,可以引入专家的知识,通过一定的高效方式迅速建立起物品的相关度表</li></ul><h2 id="3-2-利用用户注册信息"><a href="#3-2-利用用户注册信息" class="headerlink" title="3.2 利用用户注册信息"></a>3.2 利用用户注册信息</h2><p>对于新用户,我们只能推荐一些热门商品,但若知道是一名女性,就能推荐女性都喜欢的物品,即使这种个性化的粒度很粗,每个新注册的女性看到的都是相同的结果.</p><p>绝大数网站中,年龄性别都是注册用户的必备信息,还有生日,邮编等.用户的注册信息分三种:</p><ul><li>人口统计学信息    包括用户的年龄,性别,职业,民族,学历和居住地等</li><li>用户兴趣的描述    让用户用文字描述他们的兴趣</li><li>从其它网站导入的用户站外行为数据</li></ul><p>如何通过人口统计学信息给用户提供粗粒度的个性化推荐:</p><ol><li>获取用户的注册信息</li><li>根据用户的注册信息对用户分类</li><li>给用户推荐他所属分类中用户喜欢的物品</li></ol><p>例如:对于一个28岁的男性物理学家,我们可以查询三张离线计算好的相关表(年龄,性别,职业),分别查询28岁用户,男性,物理学家最喜欢的电视剧,然后将这三张相关表查询出的电视剧列表按照一定权重相加,得到最终的推荐列表.</p><p>在实际应用中,可以考虑组合特征,比如将年龄性别看作一个特征.但要注意,使用组合时用户不一定具有所有的特征,因为注册系统并不要求用户填写所有注册项.</p><p>从上面的例子可以看出,基于用户注册信息的推荐算法其核心问题是对于每种特征$f$,计算具有这种特征的用户对各个物品($i$)的喜好程度:<br>$$<br>p(f,i)&#x3D;|N(i)\cap U(f)|\ N(i):喜欢物品i的用户集合\qquad U(f):具有特征f的用户集合<br>$$<br>在这种定义下,热门的物品会在各种特征的用户中都具有比较高的权重,即高$N(i)$会导致每一类($f$)用户中都有比较高的$p(f,i)$.这又回到本质问题上,推荐系统的首要任务不是推荐热门物品,而是帮助用户发现他们不容易发现的物品.需要找一种新的定义方式来消除$N(i)$的影响,一种自然的想法就是除以$N(i)$</p><p>因此,将$p(f,i)$定义为<br>$$<br>p(f,i)&#x3D;\frac{|N(i)\cap U(f)|}{|N(i)|+\alpha}<br>$$<br>$\alpha$解决数据稀疏问题,考虑一种情况:某个物品只被一个用户喜欢过$N(i)&#x3D;1$,刚好这个用户具有特征$f$,此时$p(f,i)&#x3D;1$,但并无统计意义,因此给分母加上$\alpha$,避免该物品产生较大的比重</p><p>刚开始我的想法是</p><p>$p(f,i)&#x3D;\frac{|N(i)\cap U(f)|}{|U(f)|+\alpha}$,但其实这样推荐的还是热门物品,不过是每个特征人群中的热门物品,依然不满足推荐的本质要求.</p><p>所以看一个式子会不会受热门物品影响时,看他是否针对$N(i)$处理就行了.</p><p>接下来使用<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz">Lastfm数据集</a>来验证使用不同的人口统计学特征对预测用户行为的精度的影响,<a href="https://github.com/Magic-Bubble/RecommendSystemPractice/tree/master/Chapter3">参考代码链接</a></p><p>代码将数据集划分为10份,9训练1测试,在训练集上计算热门程度$p(f,i)&#x3D;|N(i)\cap U(f)|$,然后在测试集中给每一类用户推荐$p(f,i)$最高的十个物品,并计算准确率和召回率,覆盖率.</p><p>按照不同的粒度给用户分类,对比了不同的四种算法</p><ul><li>MostPopoular     推荐最热门的歌手</li><li>GenderMostPopular     给用户推荐和他同性别的用户最热门的歌手(男&#x2F;女)</li><li>AgeMostPopular     推荐同一个年龄段的歌手,以10岁为一个年龄段</li><li>CountryMostPopular     推荐和用户同一个国家的用户喜欢的歌手</li><li>DemographicMostPopular     给用户推荐和他同性别,年龄段,国家的用户喜欢的歌手</li></ul><p>在这个数据集下,结果是粒度越细效果越好,并且国家对精度的影响比年龄和性别的影响更大</p><h2 id="3-3-选择合适的物品启动用户的兴趣"><a href="#3-3-选择合适的物品启动用户的兴趣" class="headerlink" title="3.3 选择合适的物品启动用户的兴趣"></a>3.3 选择合适的物品启动用户的兴趣</h2><p>解决<strong>用户冷启动</strong>的另一种方法:对于新用户的第一次访问,不立即给用户展示推荐结果,而是提供一些物品让用户反馈他们对这些物品的兴趣,根据反馈结果进行个性化推荐 </p><p>问题就是选择什么样的物品让用户进行反馈?一般来说,能够用来启动用户兴趣的物品具有以下特点:</p><ul><li>比较热门     如果一开始让用户反馈的物品都比较冷门,用户不知道其情节和内容,也就无法做出准确的反馈</li><li>具有代表性和区分性      启动用户兴趣的物品不能是大众化的,否则不能区分用户个性化的兴趣</li><li>启动物品集合需要有多样性      冷启动时不知道用户的兴趣,而用户的兴趣可能非常多,因此需要提供具有很高覆盖率的启动物品集合.    e.g., 电影网站 Jinni 首先不是让用户选择喜欢哪些电影,而是先让用户选择喜欢什么类型的电影,这样就能很好的保证启动物品集合的多样性</li></ul><h3 id="用决策树的方法选择初始物品"><a href="#用决策树的方法选择初始物品" class="headerlink" title="用决策树的方法选择初始物品"></a>用决策树的方法选择初始物品</h3><p><a href="https://dl.acm.org/doi/pdf/10.1145/1935826.1935910?casa_token=9DtvT4LJCgwAAAAA:zJRFDekBIzkn14TqOfZUZYa25WJn22hh0CH4Eb4cJXJmBVGpD2aPJ5TTHGGC-PTSVLN9TBrHH6wuacs">论文地址</a></p><p><strong>决策树的构造根据:</strong></p><p>首先给定一个用户集合,作者用<strong>这群用户对物品评分的方差</strong>度量这群用户兴趣的一致程度.方差越小就说明这群用户的兴趣越一致.</p><p>对于物品$i$,将用户分为三类：$N^+(i),N^-(i),\overline {N}(i)$分别表示:喜欢物品$i$的用户集合,不喜欢物品$i$的用户集合,没有对物品$i$评分的用户集合.</p><p>令$\sigma_{u\in U’}$为用户集合$U’$中所有用户对物品评分的方差,而$\sigma_{u\in N^+(i)},\sigma_{u\in N^-(i)},\sigma_{u\in  \overline {N}(i)}$ 分别表示喜欢物品$i$的用户<strong>对其它物品</strong>评分的方差, …….通过以下方式度量一个物品$i$的<strong>区分度</strong>$D(i)$:<br>$$<br>D(i)&#x3D;\sigma_{u\in N^+(i)}+\sigma_{u\in N^-(i)}+\sigma_{u\in  \overline {N}(i)}<br>$$<br>如果这三类用户集合内的用户对其他的物品兴趣很不一致，说明物品$i$具有较高的区分度.</p><p><strong>决策树的构造:</strong></p><p>首先从所有用户中找到具有最高区分度的物品$i$,然后将用户分成3类.在每类用户中再找到最具区分度的物品,再分为三类,此时总用户被分为9类,如此进行下去,从而可以通过用户对一系列物品的看法将用户分类.</p><p>在冷启动时,可以从根节点开始询问用户对该节点物品的看法,然后根据用户的选择将用户放到不同的分支,直到进入最后的叶子节点,此时我们就已经对用户的兴趣有了比较清楚的了解,从而可以对用户进行比较准确的个性化推荐.</p><p>e.g.,</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220330164621765.png" alt="image-20220330164621765" style="zoom: 67%;" /></div><h2 id="3-4-利用物品的内容消息"><a href="#3-4-利用物品的内容消息" class="headerlink" title="3.4 利用物品的内容消息"></a>3.4 利用物品的内容消息</h2><p>之前的用户冷启动关注的是如何给新用户做个性化推荐,而物品冷启动关注的是如何将新的物品推荐给可能对它感兴趣的用户这一问题.显然,物品冷启动在新闻网站等时效性很强的网站中非常重要,因为每时每刻都有新加入的物品,并且要第一时间展现给相应的用户,否则过一段时间,物品的价值就会大大降低.</p><h3 id="UserCF算法："><a href="#UserCF算法：" class="headerlink" title="UserCF算法："></a>UserCF算法：</h3><p>对于大多数网站，推荐列表并不是展示内容的唯一列表，每当一个新物品加入，总会有其他用户通过其他途径看到这些产品并反馈，那么和他历史兴趣相似的其他用户的推荐列表中就有可能出现这种产品，这一过程可以不断进行下去，该物品救恩那个逐步展示到对它感兴趣用户的推荐列表中。</p><p>但有些网站的推荐列表可能是用户获取信息的主要途径，此时$UserCF$算法就必须解决第一推动力的问题,即第一个用户从哪发现新的物品.解决第一推动力最简单的方法是考虑利用物品的<strong>内容信息</strong>,将新物品优先投放给曾经喜欢过和它内容相似的其他物品的用户.</p><h3 id="ItemCF算法："><a href="#ItemCF算法：" class="headerlink" title="ItemCF算法："></a>ItemCF算法：</h3><p>对$ItemCF$算法来说，物品冷启动比较严重，因为其原理是给用户推荐和他之前喜欢的物品相似的物品。$ItemCF$算法每隔一段时间会利用用户行为计算物品相似度，在线服务时会将之前计算好的物品相似度矩阵放在内存中。当新物品加入时，内存中的物品相关表没有该信息，所以无法推荐新物品。解决办法是频繁更新物品相似度表：</p><ul><li><p><strong>基于用户行为计算</strong>物品相似度：两个问题：</p><ul><li>用户行为日志庞大，计算矩阵十分耗时；</li><li>新用品不展示给用户就无法产生行为日志,也就计算不出包含新物品的相关矩阵。</li></ul></li><li><p><strong>利用物品的内容信息计算</strong>物品相关表，并频繁地更新相关表。</p><ul><li>我的理解：线上服务时，每当有新的物品，先利用内容相似度推荐物品；到了线下，再利用用户行为日志（此时的用户行为日志是包含了利用内容相似度推荐的新物品的行为日志）计算相似度矩阵，到了下一次线上服务，这些新物品已经在相似度矩阵中了，也就是说不再是新物品，如此循环往复下去。</li></ul></li></ul><p>物品的内容信息多种多样，不同类型的物品有不同的内容信息，如下表：</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220330203545360.png" alt="image-20220330164621765" style="zoom: 67%;" /></div><h3 id="内容信息（内容相似度）"><a href="#内容信息（内容相似度）" class="headerlink" title="内容信息（内容相似度）:"></a>内容信息（内容相似度）:</h3><p><a href="https://www.zhihu.com/question/19971859">协同过滤和基于内容推荐有什么区别？</a></p><p><u>协同过滤纯粹基于用户的历史行为，跟物品的本身属性，内容没有任何关系，而基于内容则是一句用户的历史行为中操作的过的物品，推荐跟这些物品本身相似的物品。</u>（当然，处理$ItemCF$冷启动时用到了一些内容相似度）</p><p>物品的内容可以通过向量空间模型表示，其将物品表示为一个关键词向量.如果物品的内容是一些如导演、演员等实体，可以直接将这些实体作为关键词；但如果内容是文本的形式，则需要对关键词进行抽取。对于中文，首先要对文本进行分词，将字流变为词流，然后从词流中检测出命名实体，这些实体和一些其他重要的词组成关键词集合，最后对关键词进行排名，计算每个关键词的权重，从而生成关键词向量。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220331120846736.png" alt="图3-11 关键词向量的生成过程" style="zoom: 77%;" /></div><p>对物品d，它的内容表示成一个关键词向量如下：<br>$$<br>d_i&#x3D;{(e_1,w_1),(e_2,w_2),…}\qquad e_i是关键词\qquad w_i是关键词对应的权重<br>$$<br>可以用TF-IDF公式计算词的权重：<br>$$<br>w_i&#x3D;\frac{TF(e_i)}{logDF(e_i)}<br>$$<br>对于电影这类非文本物品，也可以根据演员在剧中的重要程度赋予他们权重。</p><p>给定物品内容的关键词向量后，物品的内容相似度可以通过过向量之间的余弦相似度计算：<br>$$<br>w_{ij}&#x3D;\frac{d_i\cdot d_j}{\sqrt{\Vert d_i\Vert \Vert d_j\Vert}}<br>$$<br>设文档集合$D$有$N$个物品，每个物品平均由$m$个实体表示，如果两两计算相似度，复杂度过高$O(N^2m)$，因此实际使用中一般通过建立关键词-物品的倒排表加速这一过程。</p><p>得到物品相似度后，便可使用$ItemCF$进行推荐。</p><h3 id="物品的内容相似度-VS-协同过滤"><a href="#物品的内容相似度-VS-协同过滤" class="headerlink" title="物品的内容相似度 VS 协同过滤"></a>物品的内容相似度 VS 协同过滤</h3><p>内容相似度计算简单，能频繁更新，而且能解决冷启动问题，但丢失了一些诸如用户行为中包含的规律，物品流行度等信息。所以一般情况下$ItemCF$的效果更好，但当用户的行为强烈受某一内容属性的影响（例如github中开源项目的作者，人们往往会关注同一个作者的不同项目），内容过滤算法的精度也很高。</p><p>向量空间模型在内容数据丰富时可获得比较好的效果，例如长文本的相似度；但文本很短，关键词很少时，该模型就很难计算出<strong>准确</strong>的相似度。如下例：</p><p>“推荐系统的动态特性”和“基于时间的协同过滤算法研究”显然是类似的文章，但是标题中没有一样的关键词。为了解决关键词不同但关键词所属话题相同的问题。首先需要知道文章的话题分布，然后才能准确的计算文章的相似度。<strong>话题模型（topic model）</strong>的一大重点就是建立文章、话题和关键词的关系。代表性的话题模型有LDA。</p><h3 id="LDA（Latent-Dirichlet-Allocation）"><a href="#LDA（Latent-Dirichlet-Allocation）" class="headerlink" title="LDA（Latent Dirichlet Allocation）"></a>LDA（Latent Dirichlet Allocation）</h3><p>LDA模型对一篇文档的产生过程进行了建模，其基本思想是：一个人在写一篇文档的时候，会首先想这篇文章要讨论哪些话题，然后思考这些话题应该用什么词描述，从而最终用词写成一篇文章。因此，<strong>文章和词之间是通过话题联系的。</strong></p><p>在使用LDA计算物品的内容相似度时，可以先计算出物品在话题上的分布，然后利用两个物品的话题分布计算物品的相似度。计算分布的相似度可利用$KL$散度：<br>$$<br>D_{KL}(p||q)&#x3D;\sum_ip(i)ln\frac{p(i)}{q(i)}<br>$$<br>其中$p$和$q$是两个<strong>分布</strong>，$KL$散度越大说明分布的相似度越低。</p><h4 id="LDA模型："><a href="#LDA模型：" class="headerlink" title="LDA模型："></a>LDA模型：</h4><p>参考统计学习方法第二版第20章和 <a href="https://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6.pdf">LDA数学八卦</a>（<strong>Highly recommend</strong>）</p><h5 id="0-1-文本建模"><a href="#0-1-文本建模" class="headerlink" title="0.1 文本建模"></a>0.1 文本建模</h5><p>我们日常生活中总是产生大量的文本，如果每个文本存储为一篇文档，那每篇文档从人的观察来说就是有序的词的序列$d&#x3D;(w_1,w_2,…,w_n)$</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407100937128.png" alt="image-20220407100937128" style="zoom: 50%;" /></div><p>统计文本建模的目的就是追问这些<strong>观察到</strong>语料库的词序列是如何生成的.我们可以把人类产生的所有语料文本看成是上帝掷骰子产生的,我们观察到的只是上帝玩这个游戏的结果,所以在统计文本建模中,我们希望猜测出上帝是如何玩这个游戏的,具体就是如下两个问题:</p><ol><li>上帝都有什么样的骰子  (模型中都有哪些参数,骰子的每个面的概率都对应于模型中的参数)</li><li>上帝是如何掷这些骰子的  (游戏规则是什么,上帝可能有各种不同类型的骰子,上帝可以按照一定的规则抛掷这些骰子从而产生词序列)</li></ol><h6 id="0-1-1-Unigram-Model"><a href="#0-1-1-Unigram-Model" class="headerlink" title="0.1.1 Unigram Model"></a>0.1.1 Unigram Model</h6><p>假设词典中一共有$V$个词$v_1,v_2,…,v_V$,则最简单的Unigram Model就是认为上帝是按照如下的游戏规则产生文本的:</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407103111342.png" alt="image-20220407103111342" style="zoom:80%;" /></div><p>上帝的这个<strong>唯一</strong>的骰子各个面的概率记为$\vec p&#x3D;(p_1,p_2,…,p_v)$，所以每次投掷骰子类似于一个抛钢镚时候的贝努利实验，只是贝努利实验中我们抛的是一个两面的骰子，而此处抛的是一个$V$面的骰子，我们把抛这个$V$面骰子的实验记为记为$w\sim Mult(w|\vec p)$</p><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407103832790.png" alt="image-20220407103832790" style="zoom:80%;" /><p>对于以上模型，贝叶斯统计学派的统计学家会有不同意见，他们会很挑剔的批评只假设上帝拥有唯一一个固定的骰子是不合理的。在贝叶斯学派看来，一切参数都是随机变量，以上模型中的骰子不是唯一固定的，它也是一个随机变量。所以按照贝叶斯学派的观点，上帝是按照以下的过程在玩游戏的</p><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407105135319.png" alt="image-20220407105135319" style="zoom:80%;" /><p>上帝的这个坛子里面，骰子可以是无穷多个，有些类型的骰子数量多，有些类型的骰子少，所以从概率分布的角度看，坛子里面的骰子$p$服从一个概率分布$p(\vec p)$，这个分布称为参数$\vec p $的先验分布。</p><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407105531031.png" alt="image-20220407105531031" style="zoom:67%;" /><h6 id="0-1-2-Topic-Model和PLSA"><a href="#0-1-2-Topic-Model和PLSA" class="headerlink" title="0.1.2 Topic Model和PLSA"></a>0.1.2 Topic Model和PLSA</h6><p>Hoffman 认为一篇文档（Document）可以由多个主题（Topic）混合而成，而每个Topic都是词汇上的概率分布，文章中的每个词都是由一个固定的Topic生成的。下图是英语中几个Topic的例子:</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220406224309170.png" alt="image-20220406224309170"  /></div><p>所有人类思考和写文章的行为都可以认为是上帝的行为,那么在PLSA模型中,Hoffman 认为上帝是按照如下的游戏规则来生成文本的:</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407114036121.png" alt="image-20220407114036121" style="zoom:80%;" /></div><p>PLSA模型的文档生成过程可图形化的表示为:</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407114142439.png" alt=" " style="zoom:80%;" /></div><p>我们可以发现在以上的游戏规则下，文档和文档之间是独立可交换的，同一个文档内的词也是独立可交换的，还是一个bag-of-words模型。游戏中的$K$个topic-word骰子，我们可以记$\vec \varphi_1,…,\vec \varphi_K,$对于包含$M$篇文档的语料$C&#x3D;(d_1,d_2,..,d_M)$中的每篇文档$d_m$，都会有一个特定的doc-topic骰子$\vec \theta_m$，所有对应的骰子记为$\vec \theta_1,…,\vec \theta_M$,为了方便，我们假设每个词$w$都是一个编号，对应到topic-word骰子的面。于是在PLSA这个模型中，第$m$篇文档$d_m$中的每个词的生成概率为<br>$$<br>p(w|d_m)&#x3D;\sum_{z&#x3D;1}^Kp(w|z)p(z|d_m)&#x3D;\sum_{z&#x3D;1}^K\varphi_{zw}\theta_{mz}<br>$$</p><h5 id="0-2-LDA文本建模"><a href="#0-2-LDA文本建模" class="headerlink" title="0.2 LDA文本建模"></a>0.2 LDA文本建模</h5><h6 id="0-2-1-游戏规则"><a href="#0-2-1-游戏规则" class="headerlink" title="0.2.1 游戏规则"></a>0.2.1 游戏规则</h6><p>对于上述的PLSA模型，贝叶斯学派显然是有意见的，doc-topic骰子$\vec \theta_m$和topic-word骰子$\vec \varphi_k$都是模型中的参数，参数都是随机变量，怎么能没有先验分布呢？于是，类似于对Unigram Model的贝叶斯改造，我们也可以如下在两个骰子参数前加上先验分布从而把PLSA对应的游戏过程改造为一个贝叶斯的游戏过程。由于$\vec \theta_m$和$\vec \varphi_k$都对应到多项分布，所以先验分布的一个好的选择就是Drichlet分布，于是我们就得到了LDA（Latent Dirichlet Allocation）模型。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407121414109.png" alt="image-20220407121414109" style="zoom: 67%;" /></div><p>在LDA模型中，上帝是按照如下的规则玩文档生成的游戏的:</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407121522277.png" alt="image-20220407121522277" style="zoom:75%;" /></div><h6 id="0-2-2-物理过程分解"><a href="#0-2-2-物理过程分解" class="headerlink" title="0.2.2 物理过程分解"></a>0.2.2 物理过程分解</h6><p>使用概率图模型表示，LDA模型的游戏过程如图所示:</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407121907367.png" alt="image-20220407121907367" style="zoom:33%;" /></div><p>这个概率图可以分解为两个主要的物理过程：</p><ol><li>$\vec \alpha\rightarrow\vec \theta_m\rightarrow z_{m,n}$  这个过程表示在生成<strong>第$m$篇</strong>文档的时候，先从第一个坛子中抽了一个doc-topic骰子$\vec \theta_m$，然后投掷这个骰子生成了文档中第$n$个词的topic编号$z_{m,n}$</li><li>$\vec \beta\rightarrow\vec \varphi_k\rightarrow w_{m,n}|k&#x3D;z_{m,n}$   这个过程表示用如下动作生成语料中第$m$文档的第$n$个词:在上帝手头的$K$个topic-word骰子$\vec \varphi_k$中，挑选编号为$k&#x3D;z_{m,n}$的那个骰子进行投掷，然后生成word $w_{m,n}$</li></ol><p>$$<br>\vec \varphi_k&#x3D;(\varphi_{k1},\varphi_{k2},…,\varphi_{kV})\quad \sum_{i&#x3D;1}^V\varphi_{ki}&#x3D;1\\vec \theta_m&#x3D;(\theta_{m1},\theta_{m2},…,\theta_{mK})\quad \sum_{i&#x3D;1}^K\theta_{mi}&#x3D;1\<br>V:单词个数\quad K:topic个数<br>$$</p><p>玩LDA游戏的时候，上帝是先完全处理完成一篇文档，再处理下一篇文档。文档中每个词的生成都要抛两次骰子，第一次抛一个doc-topic骰子得到topic，第二次抛一个topic-word骰子得到word，每次生成每篇文档中的一个词的时候这两次抛骰子的动作是紧邻轮换进行的。如果语料中一共有$N$个词，则上帝一共要抛$2N$次骰子，轮换的抛doc-topic骰子和topic-word骰子。但实际上有一些抛骰子的顺序是可以交换的，我们可以等价的调整$2N$次抛骰子的次序：前N次只抛doc-topic骰子得到语料中所有词的topics，然后基于得到的每个词的topic编号，后$N$次只抛topic-word骰子生成$N$个word。于是上帝在玩LDA游戏的时候，可以等价的按照如下过程进行：</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407124045680.png" alt="image-20220407124045680" style="zoom:80%;" /></div><hr><h6 id="统计学习方法中的解释-上面的两张图可以和下图一起食用"><a href="#统计学习方法中的解释-上面的两张图可以和下图一起食用" class="headerlink" title="统计学习方法中的解释:(上面的两张图可以和下图一起食用:)"></a>统计学习方法中的解释:(上面的两张图可以和下图一起食用:)</h6><p>LDA模型表示文本集合的自动生成过程：首先，基于单词分布的先验分布（狄利克雷分布）生成多个单词分布，即决定多个话题内容；之后，基于话题分布的先验分布（狄利克雷分布）生成多个话题分布，即决定多个文本内容；然后，基于每一个话题分布生成话题序列，针对每一个话题，基于话题的单词分布生成单词，整体构成一个单词序列，即生成文本，重复这个过程生成所有文本。文本的单词序列是观测变量，文本的话题序列是隐变量，文本的话题分布和话题的单词分布也是隐变量。图20.3示意LDA的文本生成过程（详见文前彩图）。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220407130339226.png" alt="image-20220407130339226" style="zoom:80%;" /></div><p>“单词分布的先验分布”对应第二个坛子中骰子的分布;“基于单词分布的先验分布（狄利克雷分布）生成多个单词分布，即决定多个话题内容”对应在第二个坛子抽出$K$个骰子.</p><p>“话题分布的先验分布”对应第一个坛子中骰子的分布;“基于话题分布的先验分布（狄利克雷分布）生成多个话题分布，即决定多个文本内容”对应在每次在第一个坛子抽1个骰子，抽$M$次.</p><hr><h6 id="0-2-3-Gibbs-Sampling"><a href="#0-2-3-Gibbs-Sampling" class="headerlink" title="0.2.3 Gibbs Sampling"></a>0.2.3 Gibbs Sampling</h6><p>模型的训练和推断在 <a href="https://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6.pdf">LDA数学八卦</a>有详细介绍,不再赘述.</p><h2 id="3-5-发挥专家的作用"><a href="#3-5-发挥专家的作用" class="headerlink" title="3.5 发挥专家的作用"></a>3.5 发挥专家的作用</h2><p>很多推荐系统在建立时，既没有用户的行为数据，也没有充足的物品内容信息来计算准确的物品相似度。因此很多系统利用专家进行标注。</p><p>以音乐推荐为例，仅利用歌曲的专辑，歌手等属性信息很难获得较好的歌曲相似度表。因为每个歌手或专辑的好作品只占少部分。因此某公司雇佣一批懂音乐的人，进行了一项称为音乐基因的项目，他们听了几万名歌手的歌，并对这些歌进行各个维度上的标注。最终，他们使用了400多个特征（也被公司称为基因）。因此，每首歌可被表示为一个400维的向量，进而使用向量相似度算法进行推荐。</p><p>当然，也可以使用半人工的方式，专家进行标记一定的样本后，使用NLU等技术，通过用户的评论和物品的属性自动标记，也可以设计让用户对基因进行反馈的页面，希望通过用户反馈不断改进电影基因系统。</p>]]></content>
    
    
    
    <tags>
      
      <tag>读书笔记</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推荐系统实践（项亮）第二章笔记</title>
    <link href="/2022/03/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
    <url>/2022/03/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="第二章-利用用户行为数据"><a href="#第二章-利用用户行为数据" class="headerlink" title="第二章 利用用户行为数据"></a>第二章 利用用户行为数据</h1><p><a href="https://github.com/Magic-Bubble/RecommendSystemPractice/tree/master/Chapter2">本章节代码来源</a></p><p>实现个性化推荐<strong>最理想的情况</strong>是用户注册的时候主动告诉我们他喜欢什么，但其缺点有三：</p><p>1)NLU技术难以理解用户描述兴趣的自然语言.</p><p>2)用户的兴趣不断变化,但用户不会不停地更新兴趣描述.</p><p>3)用户不知道自己喜欢什么&#x2F;很难用语言描述自己喜欢什么.因此,需要算法挖掘用户行为数据进而推荐.</p><p>啤酒尿布&#x2F;购物车分析的例子就说明了用户行为数据蕴含不是那么显而易见的规律,个性化推荐算法的任务就是通过计算机发现这些规律.</p><p>基于用户行为分析的推荐算法是个性化推荐系统的重要算法,学术界一般将这种类型的算法称为协同过滤算法.即用户可以不断地与网站互动,使自己的推荐列表能够不断过滤掉自己不感兴趣的物品来满足自己的需求.</p><h2 id="2-1-用户行为数据简介"><a href="#2-1-用户行为数据简介" class="headerlink" title="2.1 用户行为数据简介"></a>2.1 用户行为数据简介</h2><p>推荐系统会汇总原始日志生成描述用户行为的会话日志.这些日志记录了用户的各种行为,如在电子商务网站中这些行为主要包括网页浏览、点击、评分等.</p><p>用户行为在个性化推荐系统中一般分两种（按明确性分）</p><ul><li>显性反馈行为：明确表示对物品喜好的行为，如评分、喜欢&#x2F;不喜欢等</li><li>隐性反馈行为：不能明确反应，如页面浏览（浏览不一定是喜欢，可能只是因为在首页）</li></ul><p>按反馈的方向分为正反馈（用户行为倾向于喜欢该物品）和负反馈（反之）。</p><p>互联网行为有很多种，用统一的方式表示这些所有行为比较困难。一般不同的数据集包含不同的行为.</p><ul><li>无上下文信息的隐性反馈数据集    每条行为记录包含用户ID,物品ID.数据集Book-Crossing</li><li>无上下文信息的显性反馈数据集    每条行为记录包含用户ID,物品ID和用户对物品的评分</li><li>有上下文信息的隐性反馈数据集    每条行为记录包含用户ID,物品ID和用户对物品产生行为的时间戳.数据集Lastfm</li><li>有上下文信息的显性反馈数据集    每条行为记录包含用户ID,物品ID,评分和时间戳.数据集Netflix Prize提供的就是这种类型的数据集.</li></ul><p>本章使用的都是第一种.</p><h2 id="2-2-用户行为分析"><a href="#2-2-用户行为分析" class="headerlink" title="2.2 用户行为分析"></a>2.2 用户行为分析</h2><p>本节介绍用户行为数据中蕴含的一般规律</p><h4 id="2-2-1-用户活跃度和物品流行度的分布"><a href="#2-2-1-用户活跃度和物品流行度的分布" class="headerlink" title="2.2.1 用户活跃度和物品流行度的分布"></a>2.2.1 用户活跃度和物品流行度的分布</h4><p><strong>长尾分布</strong>——流行度大的物品占物品总数少;活跃度大的用户占用户的比例小</p><h4 id="2-2-2-用户活跃度和物品流行度的关系"><a href="#2-2-2-用户活跃度和物品流行度的关系" class="headerlink" title="2.2.2 用户活跃度和物品流行度的关系"></a>2.2.2 用户活跃度和物品流行度的关系</h4><p>一般认为新用户倾向于浏览热门物品,而活跃用户会逐渐开始浏览冷门物品.</p><p>仅基于用户行为设计的推荐算法一般称为系统过滤算法.协同过滤算法有很多种,如<strong>基于邻域的方法</strong>(neighborhood-based),隐语义模型(latent factor model),基于图的随机游走算法(random walk on graph)</p><p>基于邻域的方法主要包含下面两种算法:</p><ul><li>基于用户的协同过滤算法    给用户推荐和他兴趣相似的其他用户喜欢的物品</li><li><strong>基于物品的协同过滤算法</strong>    给用户推荐和他之前喜欢的物品相似的物品</li></ul><h2 id="2-3-实验设计和算法评测"><a href="#2-3-实验设计和算法评测" class="headerlink" title="2.3 实验设计和算法评测"></a>2.3 实验设计和算法评测</h2><h4 id="2-3-1-数据集"><a href="#2-3-1-数据集" class="headerlink" title="2.3.1 数据集"></a>2.3.1 数据集</h4><p>本实验采用GroupLens提供的中等大小版本的MovieLens数据集.</p><p>这是一个评分数据集(1~5分),包含6000多用户对4000多部电影的100万条评分.</p><p>由于本章研究<strong>隐反馈数据集中的TopN推荐</strong>问题,因此忽略数据集的评分记录(TopN推荐的任务是预测用户会不会评分,而非预测在其准备评分的前提下评多少分).</p><h4 id="2-3-2-实验设计"><a href="#2-3-2-实验设计" class="headerlink" title="2.3.2 实验设计"></a>2.3.2 实验设计</h4><p>使用K_Fold分割数据集</p><p>文件结构:</p><figure class="highlight dns"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs dns">├── ml-<span class="hljs-number">1m</span>/<br>    ├── movies.dat     // MovieI<span class="hljs-number">D::</span>Titl<span class="hljs-number">e::</span>Genres<br>    ├── ratings.dat    // UserI<span class="hljs-number">D::</span>MovieI<span class="hljs-number">D::</span>Rating<span class="hljs-number">::</span>Timestamp<br>    ├── users.dat      // UserI<span class="hljs-number">D::</span>Gender<span class="hljs-number">::</span>Ag<span class="hljs-number">e::</span>Occupation<span class="hljs-number">::</span>Zip-code<br></code></pre></td></tr></table></figure><p><strong>对数据集进行划分:</strong></p><p>本实验使用的是<code>ratings.dat</code>文件,首先对其进行预处理:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 定义装饰器，监控运行时间</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">timmer</span>(<span class="hljs-params">func</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">wrapper</span>(<span class="hljs-params">*args, **kwargs</span>):<br>        start_time = time.time()<br>        res = func(*args, **kwargs)<br>        stop_time = time.time()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Func %s, run time: %s&#x27;</span> % (func.__name__, stop_time - start_time))<br>        <span class="hljs-keyword">return</span> res<br>    <span class="hljs-keyword">return</span> wrapper<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 处理数据:</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Dataset</span>():<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, fp</span>):<br>        <span class="hljs-comment"># fp: data file path</span><br>        self.data = self.loadData(fp)<br>    <br><span class="hljs-meta">    @timmer</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loadData</span>(<span class="hljs-params">self, fp</span>):<br>        data = []<br>        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(fp):<br>            data.append(<span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>, l.strip().split(<span class="hljs-string">&#x27;::&#x27;</span>)[:<span class="hljs-number">2</span>])))<br>        <span class="hljs-keyword">return</span> data<br>    <br><span class="hljs-meta">    @timmer</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">splitData</span>(<span class="hljs-params">self, M, k, seed=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        :params: data, 加载的所有(user, item)数据条目</span><br><span class="hljs-string">        :params: M, 划分的数目，最后需要取M折的平均</span><br><span class="hljs-string">        :params: k, 本次是第几次划分，k~[0, M)</span><br><span class="hljs-string">        :params: seed, random的种子数，对于不同的k应设置成一样的</span><br><span class="hljs-string">        :return: train, test</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        train, test = [], []<br>        random.seed(seed)<br>        <span class="hljs-keyword">for</span> user, item <span class="hljs-keyword">in</span> self.data:<br>            <span class="hljs-comment"># 这里与书中的不一致，本人认为取M-1较为合理，因randint是左右都覆盖的</span><br>            <span class="hljs-keyword">if</span> random.randint(<span class="hljs-number">0</span>, M-<span class="hljs-number">1</span>) == k:  <br>                test.append((user, item))<br>            <span class="hljs-keyword">else</span>:<br>                train.append((user, item))<br><br>        <span class="hljs-comment"># 处理成字典的形式，user-&gt;set(items)</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_dict</span>(<span class="hljs-params">data</span>):<br>            data_dict = &#123;&#125;<br>            <span class="hljs-keyword">for</span> user, item <span class="hljs-keyword">in</span> data:<br>                <span class="hljs-keyword">if</span> user <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data_dict:<br>                    data_dict[user] = <span class="hljs-built_in">set</span>()<br>                data_dict[user].add(item)<br>            data_dict = &#123;k: <span class="hljs-built_in">list</span>(data_dict[k]) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> data_dict&#125;<br>            <span class="hljs-keyword">return</span> data_dict<br><br>        <span class="hljs-keyword">return</span> convert_dict(train), convert_dict(test)<br></code></pre></td></tr></table></figure><p>每次实验选取不同的$k(0\leq k\leq M-1)$和<strong>相同的seed</strong>，由于相同的seed产生的随机数序列一样，且范围在$0\sim M-1$，当选取不同的$k$时,得到的test也不同,且test和$k$一一对应,因此能得到$M$个训练&#x2F;测试集.</p><p>这样做是为了防止某次实验过拟合.</p><p>经上述步骤处理后数据格式如下,表示用户ID1看过的电影为ID608,ID3114,ID1246.用户ID2看过的是ID1537和ID2628.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml">&#123;<span class="hljs-attr">1:</span> [<span class="hljs-number">608</span>, <span class="hljs-number">3114</span>, <span class="hljs-number">1246</span>], <span class="hljs-attr">2:</span> [<span class="hljs-number">1537</span>, <span class="hljs-number">2628</span>]&#125;<br></code></pre></td></tr></table></figure><h4 id="2-3-3-评测指标"><a href="#2-3-3-评测指标" class="headerlink" title="2.3.3 评测指标"></a>2.3.3 评测指标</h4><p>对用户$u$推荐$N$个物品(记为$R(U)$),令该用户在测试集上喜欢的物品集合为$T(U)$,可通过$precision$和$recall$评测算法精度:</p><ul><li><p>召回率:label中的评分记录有多少在预测的推荐列表中,因此分母是label中的正样本数.</p></li><li><p>准确率:预测的推荐列表中有多少是发生过的评分记录</p></li></ul><p>e.g.$+$表示label&#x2F;predict中用户喜欢的物品</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220316235946240.png" alt="" style="zoom:50%;" /></div><p>此例有:</p><p>$|U|&#x3D;1(用户总数),N&#x3D;7(推荐列表长度,在这里假设label长度也是7)$,</p><p>$R(U)\cap T(U)&#x3D;4,R(U)&#x3D;7,T(U)&#x3D;5$</p><p>$\Rightarrow recall&#x3D;\frac{\sum_{u\in U}|R(U)\cap T(U)|}{\sum_{u\in U}T(U)}&#x3D;\frac{4}{5},precision&#x3D;\frac{\sum_{u\in U}|R(U)\cap T(U|}{\sum_{u\in U}R(U)}&#x3D;\frac{4}{7}$</p><p>再次强调,该实验的目的是<strong>预测用户会不会评分</strong></p><ul><li><p>覆盖率$Coverage&#x3D;\frac{\cup_{u\in U}\ R(u)}{|I|}$</p></li><li><p>新颖度,在这里使用平均流行度<strong>度量</strong>推荐结果的新颖度,平均流行度越高,新颖度越低.</p></li></ul><p>由于物品流行度分布满足长尾分布,因此在计算平均流行度时对每个物品的流行度取对数,使流行度平均值更加稳定.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Metric</span>():<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train, test, GetRecommendation</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        :params: train, 训练数据</span><br><span class="hljs-string">        :params: test, 测试数据</span><br><span class="hljs-string">        :params: GetRecommendation, 为某个用户获取推荐物品的接口函数</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        self.train = train<br>        self.test = test<br>        self.GetRecommendation = GetRecommendation<br>        self.recs = self.getRec()<br>        <br>    <span class="hljs-comment"># 为test中的每个用户进行推荐</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">getRec</span>(<span class="hljs-params">self</span>):<br>        recs = &#123;&#125;<br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> self.test:<br>            rank = self.GetRecommendation(user)<br>            recs[user] = rank<br>        <span class="hljs-keyword">return</span> recs<br>        <br>    <span class="hljs-comment"># 定义精确率指标计算方式</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">precision</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">all</span>, hit = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> self.test:<br>            test_items = <span class="hljs-built_in">set</span>(self.test[user])<br>            rank = self.recs[user]<br>            <span class="hljs-keyword">for</span> item, score <span class="hljs-keyword">in</span> rank:<br>                <span class="hljs-keyword">if</span> item <span class="hljs-keyword">in</span> test_items:<br>                    hit += <span class="hljs-number">1</span><br>            <span class="hljs-built_in">all</span> += <span class="hljs-built_in">len</span>(rank)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(hit / <span class="hljs-built_in">all</span> * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-comment"># 定义召回率指标计算方式</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">recall</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">all</span>, hit = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> self.test:<br>            test_items = <span class="hljs-built_in">set</span>(self.test[user])<br>            rank = self.recs[user]<br>            <span class="hljs-keyword">for</span> item, score <span class="hljs-keyword">in</span> rank:<br>                <span class="hljs-keyword">if</span> item <span class="hljs-keyword">in</span> test_items:<br>                    hit += <span class="hljs-number">1</span><br>            <span class="hljs-built_in">all</span> += <span class="hljs-built_in">len</span>(test_items)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(hit / <span class="hljs-built_in">all</span> * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-comment"># 定义覆盖率指标计算方式</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">coverage</span>(<span class="hljs-params">self</span>):<br>        all_item, recom_item = <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>()<br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> self.test:<br>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> self.train[user]:<br>                all_item.add(item)<br>            rank = self.recs[user]<br>            <span class="hljs-keyword">for</span> item, score <span class="hljs-keyword">in</span> rank:<br>                recom_item.add(item)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(<span class="hljs-built_in">len</span>(recom_item) / <span class="hljs-built_in">len</span>(all_item) * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-comment"># 定义新颖度指标计算方式</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">popularity</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 计算物品的流行度</span><br>        item_pop = &#123;&#125;<br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> self.train:<br>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> self.train[user]:<br>                <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> item_pop:<br>                    item_pop[item] = <span class="hljs-number">0</span><br>                item_pop[item] += <span class="hljs-number">1</span><br><br>        num, pop = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> self.test:<br>            rank = self.recs[user]<br>            <span class="hljs-keyword">for</span> item, score <span class="hljs-keyword">in</span> rank:<br>                <span class="hljs-comment"># 取对数，防止因长尾问题带来的被流行物品所主导</span><br>                pop += math.log(<span class="hljs-number">1</span> + item_pop[item])<br>                num += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(pop / num, <span class="hljs-number">6</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">eval</span>(<span class="hljs-params">self</span>):<br>        metric = &#123;<span class="hljs-string">&#x27;Precision&#x27;</span>: self.precision(),<br>                  <span class="hljs-string">&#x27;Recall&#x27;</span>: self.recall(),<br>                  <span class="hljs-string">&#x27;Coverage&#x27;</span>: self.coverage(),<br>                  <span class="hljs-string">&#x27;Popularity&#x27;</span>: self.popularity()&#125;<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Metric:&#x27;</span>, metric)<br>        <span class="hljs-keyword">return</span> metric<br></code></pre></td></tr></table></figure><h2 id="2-4-基于邻域的算法"><a href="#2-4-基于邻域的算法" class="headerlink" title="2.4 基于邻域的算法"></a>2.4 基于邻域的算法</h2><p>分为两种,基于<strong>用户</strong>的协同过滤算法和基于<strong>物品</strong>的协同过滤算法</p><h4 id="2-4-1-基于用户的协同过滤算法-UserCF"><a href="#2-4-1-基于用户的协同过滤算法-UserCF" class="headerlink" title="2.4.1 基于用户的协同过滤算法(UserCF)"></a>2.4.1 基于用户的协同过滤算法(UserCF)</h4><h5 id="1-基础算法"><a href="#1-基础算法" class="headerlink" title="1. 基础算法"></a>1. 基础算法</h5><p>两个步骤:</p><ol><li>找到和目标用户兴趣相似的用户集合</li><li>找到这个集合中用户喜欢的,且目标用户没听说过的物品推荐给目标用户</li></ol><p>步骤1的关键是计算两个用户的兴趣相似度,而协同算法主要利用行为的相似度计算兴趣的相似度.令$N(u)$表示用户$u$曾经有过正反馈的物品集合,$N(v)$为用户$v$曾经有过正反馈的物品集合.两者的用户相似度可通过下面的$Jaccard$公式计算:<br>$$<br>w_{uv}&#x3D;\frac{|N(u)\cap N(v)|}{|N(u)||N(v)|}<br>$$<br>也可通过余弦相似度计算:<br>$$<br>w_{uv}&#x3D;\frac{|N(u)\cap N(v)|}{\sqrt{|N(u)||N(v)|}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">UserSimilarity</span>(<span class="hljs-params">train</span>):<br>    W=<span class="hljs-built_in">dict</span>()<br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> train.keys():<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> train.keys():<br>            <span class="hljs-keyword">if</span> u==v:<br>                <span class="hljs-keyword">continue</span><br>            W[u][v]=<span class="hljs-built_in">len</span>(train[u]&amp;train[v])<br>            W[u][v]/=math.sqrt(<span class="hljs-built_in">len</span>(train[u])*<span class="hljs-built_in">len</span>(train[v])*<span class="hljs-number">1.0</span>)<br>    <span class="hljs-keyword">return</span> W<br></code></pre></td></tr></table></figure><p>这种计算相似度矩阵的方法时间复杂度为$O(|U|*|U|)$,然而大多数用户之间并没有共同购买的商品,即$|N(u)\cap N(v)|&#x3D;0$,这会加长计算时间.</p><p>下面这种计算相似度的算法首先建立物品到用户的<strong>倒查表</strong>,对于每个物品,保存对该物品产生过行为的<strong>用户列表</strong>.然后建立矩阵$C_{|U|\times |V|}$.该矩阵的每个元素代表两个用户间共同交互物品的个数,即$|N(u)\cap N(v)|$,可以先计算出$|N(u)\cap N(v)|\neq 0$的用户对$(u,v)$,然后再对这种情况除以$\sqrt{|N(u)||N(v)|}$.</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220317171752948.png" alt="图2-7 物品-用户列表" style="zoom: 70%;" /></div><p>如上图,对于物品$a$,用户$A,B$都对其产生过行为,因此将$C[A][B]$和$C[B][A]$加1,以此类推.得到完整的$C$后,除以分母得到$W$.</p><p>得到用户间相似度矩阵$W$后,$\ UserCF$算法会给用户推荐和他兴趣最相似的$K$个用户喜欢的物品,用户$u$对物品$i$的感兴趣程度为:<br>$$<br>p(u,i)&#x3D;\sum_{v\in S(u,k)\cap N(i)}w_{uv}r_{vi}<br>$$<br>$S(u,k)$:与用户$u$兴趣最相似的$k$个人;    $N(i)$:对物品$i$感兴趣的用户集合</p><p>$w_{uv}$:$u,v$两个用户的兴趣相似度;    $r_{vi}$:用户$v$对物品$i$的感兴趣程度</p><p>因此,由上图可得:$\quad p(A,c)&#x3D;w_{AB}+w_{AD}&#x3D;0.7416\qquad p(A,e)&#x3D;w_{AC}+w_{AD}&#x3D;0.7416$</p><p>$UserCF$推荐算法的实现:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 3. 基于用户余弦相似度的推荐</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">UserCF</span>(<span class="hljs-params">train, K, N</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :params: train, 训练数据集</span><br><span class="hljs-string">    :params: K, 超参数，设置取TopK相似用户数目</span><br><span class="hljs-string">    :params: N, 超参数，设置取TopN推荐物品数目</span><br><span class="hljs-string">    :return: GetRecommendation, 推荐接口函数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 计算item-&gt;user的倒排索引</span><br>    item_users = &#123;&#125;<br>    <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> train:<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> train[user]:<br>            <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> item_users:<br>                item_users[item] = []<br>            item_users[item].append(user)<br>    <br>    <span class="hljs-comment"># 计算用户相似度矩阵</span><br>    sim = &#123;&#125;<br>    num = &#123;&#125;<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> item_users:<br>        users = item_users[item]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(users)):<br>            u = users[i]<br>            <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> num:<br>                num[u] = <span class="hljs-number">0</span><br>            num[u] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> sim:<br>                sim[u] = &#123;&#125;<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(users)):<br>                <span class="hljs-keyword">if</span> j == i: <span class="hljs-keyword">continue</span><br>                v = users[j]<br>                <span class="hljs-keyword">if</span> v <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> sim[u]:<br>                    sim[u][v] = <span class="hljs-number">0</span><br>                sim[u][v] += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> sim:<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sim[u]:<br>            sim[u][v] /= math.sqrt(num[u] * num[v])<br>    <br>    <span class="hljs-comment"># 按照相似度排序</span><br>    sorted_user_sim = &#123;k: <span class="hljs-built_in">list</span>(<span class="hljs-built_in">sorted</span>(v.items(), \<br>                               key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)) \<br>                       <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sim.items()&#125;<br>    <br>    <span class="hljs-comment"># 获取接口函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">GetRecommendation</span>(<span class="hljs-params">user</span>):<br>        items = &#123;&#125;<br>        seen_items = <span class="hljs-built_in">set</span>(train[user])<br>        <span class="hljs-keyword">for</span> u, _ <span class="hljs-keyword">in</span> sorted_user_sim[user][:K]:<br>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> train[u]:<br>                <span class="hljs-comment"># 要去掉用户见过的</span><br>                <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> seen_items:<br>                    <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> items:<br>                        items[item] = <span class="hljs-number">0</span><br>                    items[item] += sim[user][u]<br>        recs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">sorted</span>(items.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>))[:N]<br>        <span class="hljs-keyword">return</span> recs<br>    <br>    <span class="hljs-keyword">return</span> GetRecommendation<br></code></pre></td></tr></table></figure><p>从$UserCF$的实验结果可得出以下结论:</p><ul><li>RS的精度指标$recall,precision$与$K$不是线性关系</li><li>$K$越大,推荐结果越热门(因为参考了更多人的意见)</li><li>$K$越大,覆盖率越低</li></ul><h5 id="2-用户相似度计算的改进"><a href="#2-用户相似度计算的改进" class="headerlink" title="2. 用户相似度计算的改进"></a>2. 用户相似度计算的改进</h5><p>两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度（如果两个人都购买了《新华字典》，很难不能说明他们兴趣相似）。</p><p>$John\ S.\ Breese$提出:<br>$$<br>w_{uv}&#x3D;\frac{\sum_{i\in N(u)\cap N(v)}\frac{1}{log(1+|N(i)|)}}{\sqrt {|N(u)||N|v||}}<br>$$<br>即通过$\frac{1}{log1+|N(i)|}$惩罚用户$u$和$v$共同兴趣列表中热门物品对他们相似度的影响.</p><p>将其称为$User-IIF$算法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 4. 基于改进的用户余弦相似度的推荐</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">UserIIF</span>(<span class="hljs-params">train, K, N</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :params: train, 训练数据集</span><br><span class="hljs-string">    :params: K, 超参数，设置取TopK相似用户数目</span><br><span class="hljs-string">    :params: N, 超参数，设置取TopN推荐物品数目</span><br><span class="hljs-string">    :return: GetRecommendation, 推荐接口函数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 计算item-&gt;user的倒排索引 ...</span><br>    <span class="hljs-comment"># 计算用户相似度矩阵</span><br>    sim = &#123;&#125;<br>    num = &#123;&#125;<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> item_users:<br>        users = item_users[item]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(users)):<br>            u = users[i]<br>            <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> num:<br>                num[u] = <span class="hljs-number">0</span><br>            num[u] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> sim:<br>                sim[u] = &#123;&#125;<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(users)):<br>                <span class="hljs-keyword">if</span> j == i: <span class="hljs-keyword">continue</span><br>                v = users[j]<br>                <span class="hljs-keyword">if</span> v <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> sim[u]:<br>                    sim[u][v] = <span class="hljs-number">0</span><br>                <span class="hljs-comment"># 相比UserCF，主要是改进了这里</span><br>                sim[u][v] += <span class="hljs-number">1</span> / math.log(<span class="hljs-number">1</span> + <span class="hljs-built_in">len</span>(users))<br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> sim:<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sim[u]:<br>            sim[u][v] /= math.sqrt(num[u] * num[v])<br>    <br>    <span class="hljs-comment"># 按照相似度排序 ...</span><br>    <span class="hljs-comment"># 获取接口函数 ...</span><br>    <span class="hljs-keyword">return</span> GetRecommendation<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Experiment</span>():<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, M, K, N, fp=<span class="hljs-string">&#x27;../dataset/ml-1m/ratings.dat&#x27;</span>, rt=<span class="hljs-string">&#x27;UserCF&#x27;</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        :params: M, 进行多少次实验</span><br><span class="hljs-string">        :params: K, TopK相似用户的个数</span><br><span class="hljs-string">        :params: N, TopN推荐物品的个数</span><br><span class="hljs-string">        :params: fp, 数据文件路径</span><br><span class="hljs-string">        :params: rt, 推荐算法类型</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        self.M = M<br>        self.K = K<br>        self.N = N<br>        self.fp = fp<br>        self.rt = rt<br>        self.alg = &#123;<span class="hljs-string">&#x27;Random&#x27;</span>: Random, <span class="hljs-string">&#x27;MostPopular&#x27;</span>: MostPopular, \<br>                    <span class="hljs-string">&#x27;UserCF&#x27;</span>: UserCF, <span class="hljs-string">&#x27;UserIIF&#x27;</span>: UserIIF&#125;<br>    <br>    <span class="hljs-comment"># 定义单次实验</span><br><span class="hljs-meta">    @timmer</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">worker</span>(<span class="hljs-params">self, train, test</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        :params: train, 训练数据集</span><br><span class="hljs-string">        :params: test, 测试数据集</span><br><span class="hljs-string">        :return: 各指标的值</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        getRecommendation = self.alg[self.rt](train, self.K, self.N)<br>        metric = Metric(train, test, getRecommendation)<br>        <span class="hljs-keyword">return</span> metric.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-comment"># 多次实验取平均</span><br><span class="hljs-meta">    @timmer</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        metrics = &#123;<span class="hljs-string">&#x27;Precision&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Recall&#x27;</span>: <span class="hljs-number">0</span>, <br>                   <span class="hljs-string">&#x27;Coverage&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Popularity&#x27;</span>: <span class="hljs-number">0</span>&#125;<br>        dataset = Dataset(self.fp)<br>        <span class="hljs-keyword">for</span> ii <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.M):<br>            train, test = dataset.splitData(self.M, ii)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Experiment &#123;&#125;:&#x27;</span>.<span class="hljs-built_in">format</span>(ii))<br>            metric = self.worker(train, test)<br>            metrics = &#123;k: metrics[k]+metric[k] <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> metrics&#125;<br>        metrics = &#123;k: metrics[k] / self.M <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> metrics&#125;<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Average Result (M=&#123;&#125;, K=&#123;&#125;, N=&#123;&#125;): &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(\<br>                              self.M, self.K, self.N, metrics))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. random实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br>K = <span class="hljs-number">0</span> <span class="hljs-comment"># 为保持一致而设置，随便填一个值</span><br>random_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;Random&#x27;</span>)<br>random_exp.run()<br><span class="hljs-comment"># 2. MostPopular实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br>K = <span class="hljs-number">0</span> <span class="hljs-comment"># 为保持一致而设置，随便填一个值</span><br>mp_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;MostPopular&#x27;</span>)<br>mp_exp.run()<br><span class="hljs-comment"># 3. UserCF实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> K <span class="hljs-keyword">in</span> [<span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">40</span>, <span class="hljs-number">80</span>, <span class="hljs-number">160</span>]:<br>    cf_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;UserCF&#x27;</span>)<br>    cf_exp.run()<br><span class="hljs-comment"># 4. UserIIF实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br>K = <span class="hljs-number">80</span> <span class="hljs-comment"># 与书中保持一致</span><br>iif_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;UserIIF&#x27;</span>)<br>iif_exp.run()<br></code></pre></td></tr></table></figure><h5 id="关于实验的总结"><a href="#关于实验的总结" class="headerlink" title="关于实验的总结"></a><strong>关于实验的总结</strong></h5><ol><li><p>数据集分割的小技巧，用同样的seed</p></li><li><p>各个指标的实现，要注意</p></li><li><p>为每个用户推荐的时候是推荐他们****没有见过****的，因为测试集里面是这样的</p></li><li><p>倒排物品-用户索引，可进行时间优化</p></li><li><p>推荐的时候K和N各代表什么意思，要分开设置，先取TopK，然后取TopN</p></li></ol><p>$UserCF$存在的问题,随着网站的用户数增加,计算用户兴趣矩阵越来越困难,其运算空间复杂度和时间复杂度和用户数的增长近似于平方关系.其次,这种方法很难对推荐结果做出解释.</p><h4 id="2-4-2-基于物品的系统过滤算法-ItemCF"><a href="#2-4-2-基于物品的系统过滤算法-ItemCF" class="headerlink" title="2.4.2 基于物品的系统过滤算法(ItemCF)"></a>2.4.2 基于物品的系统过滤算法(ItemCF)</h4><p>目前业界应用最多的算法</p><h5 id="1-基础算法-1"><a href="#1-基础算法-1" class="headerlink" title="1. 基础算法"></a>1. 基础算法</h5><p>$ItemCF$给用户推荐那些和他们之前喜欢的物品相似的物品.但该算法<strong>并不利用</strong>物品的内容属性计算物品间的相似度,而是通过分析用户的行为记录来计算物品间的相似度.<strong>该算法认为,物品$A$和$B$具有很大的相似度是因为喜欢物品$A$的用户大都也喜欢物品$B$.</strong></p><p>同时该算法可以给推荐结果提供一个合理的解释,例如给用户推荐《深度学习》是因为该用户之前浏览过《机器学习》.</p><p>$ItemCF$算法主要分两步:</p><ol><li><p>计算物品间的相似度</p></li><li><p>利用物品间的相似度和用户的历史行为给用户生成推荐列表</p></li></ol><p><strong>物品相似度的定义:</strong></p><p>我们购物时常能看到”购买了该商品的用户也经常购买的其他物品”,实际上,物品的相似度定义也是如此,喜欢$i$的人中有多少喜欢$j$:<br>$$<br>w_{ij}&#x3D;\frac{|N(i)\cap N(j)|}{|N(i)|}<br>$$<br>$|N(i)|$是喜欢物品$i$的用户数,$|N(i)\cap N(j)|$是同时喜欢物品$i$和$j$的用户数.</p><p><strong>存在的问题</strong>:当$j$是热门商品时,$w_{ij}$接近1，即热门商品$j$和任意的商品$i$相似性都很高,会影响RS挖掘长尾信息.为了避免推荐出热门商品.可用下面的公式.<br>$$<br>w_{ij}&#x3D;\frac{|N(i)\cap N(j)|}{\sqrt{|N(i)||N(j)|}}<br>$$<br>这个公式惩罚了物品$j$的权重,减轻了热门物品会和很多物品相似的可能性.</p><p>从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是说每个用户都可以通过他们的历史兴趣列表给物品“贡献”相似度。这里面蕴涵着一个假设，就是每个用户的兴趣都局限在某几个方面，因此<strong>如果两个物品属于一个用户的兴趣列表，那么这两个物品可能就属于有限的几个领域，而如果两个物品属于很多用户的兴趣列表，那么它们就可能属于同一个领域，因而有很大的相似度.</strong></p><p>得到相似度后,通过以下公式计算用户$u$对一个物品$j$的兴趣:<br>$$<br>p_{uj}&#x3D;\sum_{i\in N(u)\cap S(j,k)}w_{ji}r_{ui}<br>$$<br>$r_{ui}$是用户对物品$i$的兴趣,$w_{ji}$是物品$j$与物品$i$的相似度.$N(u)$为用户$u$喜欢的物品集合,$S(j,k)$是与物品$j$最相似的$k$个物品</p><p>e.g.</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220318205142800.png" alt="图2-12 一个简单的基于物品推荐的例子" style="zoom: 33%;" /></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 基于物品余弦相似度的推荐</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ItemCF</span>(<span class="hljs-params">train, K, N</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :params: train, 训练数据集</span><br><span class="hljs-string">    :params: K, 超参数，设置取TopK相似物品数目</span><br><span class="hljs-string">    :params: N, 超参数，设置取TopN推荐物品数目</span><br><span class="hljs-string">    :return: GetRecommendation, 推荐接口函数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 计算物品相似度矩阵</span><br>    sim = &#123;&#125;<br>    num = &#123;&#125;<br>    <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> train:<br>        items = train[user]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(items)):<br>            u = items[i]<br>            <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> num:<br>                num[u] = <span class="hljs-number">0</span><br>            num[u] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> sim:<br>                sim[u] = &#123;&#125;<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(items)):<br>                <span class="hljs-keyword">if</span> j == i: <span class="hljs-keyword">continue</span><br>                v = items[j]<br>                <span class="hljs-keyword">if</span> v <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> sim[u]:<br>                    sim[u][v] = <span class="hljs-number">0</span><br>                sim[u][v] += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> sim:<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sim[u]:<br>            sim[u][v] /= math.sqrt(num[u] * num[v])<br>    <br>    <span class="hljs-comment"># 按照相似度排序</span><br>    sorted_item_sim = &#123;k: <span class="hljs-built_in">list</span>(<span class="hljs-built_in">sorted</span>(v.items(), \<br>                               key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)) \<br>                       <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sim.items()&#125;<br>    <br>    <span class="hljs-comment"># 获取接口函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">GetRecommendation</span>(<span class="hljs-params">user</span>):<br>        items = &#123;&#125;<br>        seen_items = <span class="hljs-built_in">set</span>(train[user])<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> train[user]:<br>            <span class="hljs-keyword">for</span> u, _ <span class="hljs-keyword">in</span> sorted_item_sim[item][:K]:<br>                <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> seen_items:<br>                    <span class="hljs-keyword">if</span> u <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> items:<br>                        items[u] = <span class="hljs-number">0</span><br>                    items[u] += sim[item][u]<br>        recs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">sorted</span>(items.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>))[:N]<br>        <span class="hljs-keyword">return</span> recs<br>    <br>    <span class="hljs-keyword">return</span> GetRecommendation<br></code></pre></td></tr></table></figure><p>实验结果显示:</p><ul><li>精度(准确率和召回率)$\quad ItemCF$推荐结果的精度与$K$不呈正相关或负相关</li><li>流行度$\quad$和$UserCF$不同,$k$对流行度的影响不是正相关的</li><li>覆盖率$\quad K$会降低系统的覆盖率</li></ul><h5 id="2-用户活跃度对物品相似度的影响"><a href="#2-用户活跃度对物品相似度的影响" class="headerlink" title="2. 用户活跃度对物品相似度的影响"></a>2. 用户活跃度对物品相似度的影响</h5><p>假设一个开书店的人买了当当网上80%的书,意味着由于这么一个用户,有80%的书两两之间产生了相似度,也就是说内存里会产生一个很大的稠密矩阵.除此之外.该用户买这些书并非出于自身的兴趣,而且这些书覆盖了很多领域,因此该用户对于他所购买的书的两两相似度的贡献应该远远小于只买了十几本自己喜欢的书的人.</p><p>依旧是$John\ S.\ Breese$提出了$IUF(Inverse\ User\ Frequence)$,即用户活跃度对数的倒数的参数,此时活跃用户对物品相似度的贡献应该小于不活跃的用户.$N(\ast)$表示用户$\ast$有兴趣的物品集合.<br>$$<br>w_{ij}&#x3D;\frac{\sum_{u\in N(i)\cap N(j)}\frac{1}{log(1+|N(u)|)}}{\sqrt {|N(i)||N(j)|}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 只需要把上面的sim[u][v] += 1改为</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ItemIUF</span>(<span class="hljs-params">train, K, N</span>):<br>    <span class="hljs-comment"># ...</span><br>sim[u][v] += <span class="hljs-number">1</span> / math.log(<span class="hljs-number">1</span> + <span class="hljs-built_in">len</span>(items))<br>    <span class="hljs-comment"># ...</span><br></code></pre></td></tr></table></figure><p>在实际计算中,为了避免相似矩阵过于稠密,一般直接忽略该兴趣列表.</p><p>将这种算法记为$ItemCF-IUF$,实验结果显示它和$ItemCF$的精度类似,但提高了覆盖率,降低了准确度.</p><h5 id="3-物品相似度的归一化"><a href="#3-物品相似度的归一化" class="headerlink" title="3. 物品相似度的归一化"></a>3. 物品相似度的归一化</h5><p>将$ItemCF$的相似度矩阵按最大值归一化能够提高推荐的准确率,覆盖率和多样性.<br>$$<br>w_{ij}’&#x3D;\frac{w_{ij}}{\mathop{max}\limits_{j}\ w_{ij}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 3. 基于归一化的物品余弦相似度的推荐</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ItemCF_Norm</span>(<span class="hljs-params">train, K, N</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :params: train, 训练数据集</span><br><span class="hljs-string">    :params: K, 超参数，设置取TopK相似物品数目</span><br><span class="hljs-string">    :params: N, 超参数，设置取TopN推荐物品数目</span><br><span class="hljs-string">    :return: GetRecommendation, 推荐接口函数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 计算物品相似度矩阵...    </span><br>    <span class="hljs-comment"># 对相似度矩阵进行按行归一化</span><br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> sim:<br>        s = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sim[u]:<br>            s += sim[u][v]<br>        <span class="hljs-keyword">if</span> s &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sim[u]:<br>                sim[u][v] /= s<br>    <br>    <span class="hljs-comment"># 按照相似度排序 ...</span><br>    <span class="hljs-comment"># 获取接口函数 ...</span><br>    <span class="hljs-keyword">return</span> GetRecommendation<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. ItemCF实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> K <span class="hljs-keyword">in</span> [<span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">40</span>, <span class="hljs-number">80</span>, <span class="hljs-number">160</span>]:<br>    cf_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;ItemCF&#x27;</span>)<br>    cf_exp.run()<br><span class="hljs-comment"># 2. ItemIUF实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br>K = <span class="hljs-number">10</span> <span class="hljs-comment"># 与书中保持一致</span><br>iuf_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;ItemIUF&#x27;</span>)<br>iuf_exp.run()<br><span class="hljs-comment"># 3. ItemCF-Norm实验</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br>K = <span class="hljs-number">10</span> <span class="hljs-comment"># 与书中保持一致</span><br>norm_exp = Experiment(M, K, N, rt=<span class="hljs-string">&#x27;ItemCF-Norm&#x27;</span>)<br>norm_exp.run()<br></code></pre></td></tr></table></figure><p>一般来说，物品总是属于很多不同的类，每一类中的物品联系比较紧密。假设在一个电影网站中，有两种电影——纪录片和动画片。那么，ItemCF算出来的相似度一般是纪录片和纪录片的相似度或者动画片和动画片的相似度大于纪录片和动画片的相似度。<strong>但是</strong>纪录片之间的相似度和动画片之间的相似度却不一定相同。假设物品分为两类——A和B，A类物品之间的相似度为0.5，B类物品之间的相似度为0.6，而A类物品和B类物品之间的相似度是0.2。在这种情况下，如果一个用户喜欢了5个A类物品和5个B类物品，用ItemCF给他进行推荐，推荐的就都是B类物品，因为B类物品之间的相似度大。但如果归一化之后，A类物品之间的相似度变成了1，B类物品之间的相似度也是1，那么这种情况下，用户如果喜欢5个A类物品和5个B类物品，那么他的推荐列表中A类物品和B类物品的数目也应该是大致相等的。从这个例子可以看出，相似度的归一化可以提高推荐的多样性。</p><p>那么，对于两个不同的类，什么样的类其类内物品之间的相似度高或者低?</p><p>一般来说，热门的类其类内物品相似度一般比较大。如果不进行归一化，就会推荐比较热门的类里面的物品，而这些物品也是比较热门的。因此，推荐的覆盖率就比较低。相反，如果进行相似度的归一化，则可以提高推荐系统的覆盖率。</p><p>实验结果显示归一化提高了ItemCF的各项指标。</p><h4 id="2-4-3-UserCF和ItemCF的比较"><a href="#2-4-3-UserCF和ItemCF的比较" class="headerlink" title="2.4.3 UserCF和ItemCF的比较"></a>2.4.3 UserCF和ItemCF的比较</h4><p>$UserCF$是推荐系统领域较为古老的算法，1992年就已经在电子邮件的个性化推荐系统Tapestry中得到了应用，1994年被GroupLens用来实现新闻的个性化推荐，后来被著名的文章分享网站Digg用来给用户推荐个性化的网络文章。ltemCF则是相对比较新的算法，在著名的电子商务网站亚马逊和DVD租赁网站Netflix中得到了广泛应用。为什么Digg使用UserCF，而亚马逊网使用ltemCF呢？<br>首先回顾一下UserCF算法和ItemCF算法的推荐原理。UserCF给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品，而ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。从这个算法的原理可以看到，UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF的推荐结果着重于维系用户的历史兴趣。换句话说，UserCF的推荐更社会化，反映了用户所在的小型兴趣群体中物品的热门程度，而ltemCF的推荐更加个性化，反映了用户自己的兴趣传承。</p><p>在新闻网站中，用户的兴趣不是特别细化，绝大多数用户都喜欢看热门的新闻。即使是个性化，也是比较粗粒度的，比如有些用户喜欢体育新闻，有些喜欢社会新闻，而特别细粒度的个性化一般是不存在的。比方说，很少有用户只看某个话题的新闻，主要是因为这个话题不可能保证每天都有新的消息，而这个用户却是每天都要看新闻的。因此，个性化新闻推荐更加强调抓住新闻热点，热门程度和时效性是个性化新闻推荐的重点，而个性化相对于这两点略显次要。因此，UserCF可以给用户推荐和他有相似爱好的一群其他用户今天都在看的新闻，这样在抓住热点和时效性的同时，保证了一定程度的个性化。这是Digg在新闻推荐中使用UserCF的最重要原因。</p><p>UserCF适合用于新闻推荐的另一个原因是从技术角度考量的。因为作为一种物品，新闻的更新非常快，每时每刻都有新内容出现，而ltemCF需要维护一张物品相关度的表，如果物品更新很快，那么这张表也需要很快更新，这在技术上很难实现。绝大多数物品相关度表都只能做到一天一次更新，这在新闻领域是不可以接受的。而UserCF只需要用户相似性表，虽然UserCF对于新用户也需要更新相似度表，但在新闻网站中，物品的更新速度远远快于新用户的加入速度，而且对于新用户，完全可以给他推荐最热门的新闻，因此UserCF显然是利大于弊。</p><p>但是，在图书、电子商务和电影网站，比如亚马逊、豆瓣、Netflix中，ItemCF则能极大地发挥优势。首先，在这些网站中，用户的兴趣是比较固定和持久的。一个技术人员可能都是在购买技术方面的书，而且他们对书的热门程度并不是那么敏感，事实上越是资深的技术人员，他们看的书就越可能不热门。此外，这些系统中的用户大都不太需要流行度来辅助他们判断一个物品的好坏，而是可以通过自己熟悉领域的知识自己判断物品的质量。因此，这些网站中个性化推荐的任务是帮助用户发现和他研究领域相关的物品。因此，ItemCF算法成为了这些网站的首选算法。</p><p>此外，这些网站的物品更新速度不会特别快，一天一次更新物品相似度矩阵对它们来说不会造成太大的损失，是可以接受的。</p><p>同时，从技术上考虑，UserCF需要维护一个用户相似度的矩阵，而ItemCF需要维护一个物品相似度矩阵。从存储的角度说，如果用户很多，那么维护用户兴趣相似度矩阵需要很大的空间，同理，如果物品很多，那么维护物品相似度矩阵代价较大。</p><p>在早期的研究中，大部分研究人员都是让少量的用户对大量的物品进行评价，然后研究用户兴趣的模式。那么，对于他们来说，因为用户很少，计算用户兴趣相似度是最快也是最简单的方法。但在实际的互联网中，用户数目往往非常庞大，而在图书、电子商务网站中，物品的数目则是比较少的。此外，物品的相似度相对于用户的兴趣一般比较稳定，因此使用ltemCF是比较好的选择。当然，新闻网站是个例外，在那儿，物品的相似度变化很快，物品数目庞大，相反用户兴趣则相对固定（都是喜欢看热门的），所以新闻网站的个性化推荐使用UserCF算法的更多。</p><p><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220319120621302.png" alt="image-20220319120621302"></p><p><strong>该实验结果显示:$ItemCF$算法的覆盖率和新颖度没$UserCF$高,下一节会介绍如何对前者进行改进.</strong></p><p>需要指出的是，离线实验的性能在选择推荐算法时并不起决定作用。首先应该满足产品的需求，比如如果需要提供推荐解释，那么可能得选择$ltemCF$算法。其次，需要看实现代价，比如若用户太多，很难计算用户相似度矩阵，这个时候可能不得不抛弃$UserCF$算法。最后，离线指标和点击率等在线指标不一定成正比。而且，这里对比的是最原始的$UserCF$和$ItemCF$算法，这两种算法都可以进行各种各样的改进。一般来说，这两种算法经过优化后，最终得到的离线性能是近似的。</p><h5 id="哈利波特问题-原始-ItemCF-算法的局限性"><a href="#哈利波特问题-原始-ItemCF-算法的局限性" class="headerlink" title="哈利波特问题(原始$ItemCF$算法的局限性)"></a>哈利波特问题(原始$ItemCF$算法的局限性)</h5><p>亚马逊网的研究人员在设计ItemCF算法之初发现ltemCF算法计算出的图书相关表存在一个问题，就是很多书都和《哈利波特》相关。也就是说，购买任何一本书的人似乎都会购买《哈利波特》。后来他们研究发现，主要是因为《哈利波特》太热门了，确实是购买任何一本书的人几乎都会购买它。</p><p>回顾:$ItemCF$之前采用的相似度计算公式尽管考虑到了$j$是热门商品会导致的问题,但在实际应用中,$j$依然会活得比较大的相似度.<br>$$<br>w_{ij}&#x3D;\frac{|N(i)\cap N(j)|}{\sqrt{|N(i)||N(j)|}}<br>$$<br><strong>解决方案1:</strong><br>$$<br>w_{ij}&#x3D;\frac{|N(i)\cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^\alpha} \qquad \alpha\in [0.5,1]<br>$$<br>通过提高$\alpha$,京可以惩罚热门的$j$,如果$\alpha&#x3D;0.5$就是标准的ItemCF算法.</p><p>离线实验结果显示，$\alpha&#x3D;0.5$时才会导致最高的准确率和召回率，而无论$\alpha&lt;0.5$或者$\alpha&gt;0.5$都不会带来这两个指标的提高。但是，$\alpha$越大，覆盖率就越高，并且结果的平均热门程度会降低。因此，通过这种方法可以在适当牺牲准确率和召回率的情况下显著提升结果的覆盖率和新颖性(降低流行度即提高了新颖性).</p><p><strong>解决方案2:</strong></p><p>不过，上述方法还不能彻底地解决哈利波特问题。每个用户一般都会在不同的领域喜欢一种物品。以电视为例，看新闻联播是父辈每天的必修课，他们每天基本就看新闻联播，而且每天不看别的新闻，就看这一种新闻。此外，他们很多都是电视剧迷，都会看央视一套8点的电视剧。那么，最终结果就是黄金时间的电视剧都和新闻联播相似，而新闻联播和其他新闻的相似度很低。<br>上面的问题换句话说就是，<strong>两个不同领域的最热门物品之间往往具有比较高的相似度。这个时候，仅仅靠用户行为数据是不能解决这个问题的，因为用户的行为表示这种物品之间应该相似度很高。</strong>此时，我们只能<strong>依靠引入物品的内容数据</strong>解决这个问题，比如<strong>对不同领域的物品降低权重</strong>等。这些就不是协同过滤讨论的范畴了。</p><h2 id="2-5-隐语义模型"><a href="#2-5-隐语义模型" class="headerlink" title="2.5 隐语义模型"></a>2.5 隐语义模型</h2><p>LFM(latent factor model)隐语义模型最早在文本挖掘领域被提出,用于找到文本的隐含语义.</p><h4 id="2-5-1-基础算法"><a href="#2-5-1-基础算法" class="headerlink" title="2.5.1 基础算法"></a>2.5.1 基础算法</h4><p>细节可以看<a href="https://zhuanlan.zhihu.com/p/108369470">这篇文章</a></p><p>核心思想是通过隐含特征(latent factor)联系用户兴趣和物品.与$UserCF$和$ItemCF$不同,这种方法不依赖于共同评分矩阵,他将用户和物品分别映射到某种真实含义未知的特征向量(也就是说向量每个维度的意义并不能人为给定).在预测时,对于任意一个空白评分的位置,就能通过两个向量的内积进行计算.</p><p>LFM通过下式计算用户$u$对物品$i$的兴趣:<br>$$<br>Preference(u,i)&#x3D;r_{ui}&#x3D;p_u^Tq_i&#x3D;\sum_{k&#x3D;1}^Kp_{u,k}\ q_{i,k}<br>$$<br>$p_{u,k}\ q_{i,k}$是模型的参数,前者度量了用户$u$的兴趣和第$k$个隐类的关系,而$q_{i,k}$度量了第$k$个隐类和物品$i$之间的关系.</p><p>这两个参数的计算需要学习一个训练集才能得到,而且这个训练集需要包含用户喜欢和不喜欢的物品.由于本章主要讨论的是隐性反馈数据集,因此需要生成负样本.</p><p>负样本采样一般需要遵循以下原则:</p><ul><li>对每个用户,要保证正负样本的均衡</li><li>对每个用户采样负样本时,要选取那些很热门,而用户却没有行为的物品</li></ul><p>一般认为,很热门而用户却没有行为更能说明用户对该商品不感兴趣.因为对于冷门的物品,用户可能没发现,所以谈不上是否感兴趣.</p><p>经过采样可以得到一个用户-物品集$K&#x3D;{ (u,i)}$,其中如果$(u,i)$是正样本,则有$r_{ui}&#x3D;1$,否则$r_{ui}&#x3D;0$,然后需要优化以下损失函数来找到最合适的参数$p$和$q$.<br>$$<br>C&#x3D;\sum_{(u,i)\in K}(r_{ui}-\hat r_{ui})^2&#x3D;\sum_{(u,i)\in K}\Big(r_{ui}-p_u^T\ q_i\Big)^2+\lambda\Vert p_u \Vert^2+\lambda\Vert q_i \Vert^2<br>$$<br>$\lambda\Vert q_i \Vert^2$是用来防止过拟合的正则化项,$\ \lambda$可通过实验获得.<br>$$<br>\frac{\partial C}{\partial p_u}&#x3D;-2(r_{ui}-p_u^Tq_i)q_i+2\lambda p_u\<br>\frac{\partial C}{\partial q_i}&#x3D;-2(r_{ui}-p_u^Tq_i)p_u+2\lambda q_i\<br>$$<br>更新参数：<br>$$<br>p_{u}&#x3D;p_{u}+\alpha((r_{ui}-p_u^Tq_i)q_i-\lambda p_u)\<br>q_{i}&#x3D;q_{i}+\alpha((r_{ui}-p_u^Tq_i)p_u-\lambda q_i)<br>$$</p><p>$\alpha$是学习率,为超参数,通过反复实验获得.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm, trange<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LFM</span>(<span class="hljs-params">train, ratio, K, lr, step, lmbda, N</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :params: train, 训练数据</span><br><span class="hljs-string">    :params: ratio, 负采样的正负比例</span><br><span class="hljs-string">    :params: K, 隐语义个数</span><br><span class="hljs-string">    :params: lr, 初始学习率</span><br><span class="hljs-string">    :params: step, 迭代次数</span><br><span class="hljs-string">    :params: lmbda, 正则化系数</span><br><span class="hljs-string">    :params: N, 推荐TopN物品的个数</span><br><span class="hljs-string">    :return: GetRecommendation, 获取推荐结果的接口</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <br>    all_items = &#123;&#125;<br>    <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> train:<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> train[user]:<br>            <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> all_items:<br>                all_items[item] = <span class="hljs-number">0</span><br>            all_items[item] += <span class="hljs-number">1</span><br>    all_items = <span class="hljs-built_in">list</span>(all_items.items())<br>    items = [x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_items]<br>    pops = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_items]<br>    <br>    <span class="hljs-comment"># 负采样函数(注意！！！要按照流行度进行采样)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">nSample</span>(<span class="hljs-params">data, ratio</span>):<br>        new_data = &#123;&#125;<br>        <span class="hljs-comment"># 正样本</span><br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> data:<br>            <span class="hljs-keyword">if</span> user <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> new_data:<br>                new_data[user] = &#123;&#125;<br>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[user]:<br>                new_data[user][item] = <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 负样本</span><br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> new_data:<br>            seen = <span class="hljs-built_in">set</span>(new_data[user])<br>            pos_num = <span class="hljs-built_in">len</span>(seen)<br>            item = np.random.choice(items, <span class="hljs-built_in">int</span>(pos_num * ratio * <span class="hljs-number">3</span>), pops)<br>            item = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> item <span class="hljs-keyword">if</span> x <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> seen][:<span class="hljs-built_in">int</span>(pos_num * ratio)]<br>            new_data[user].update(&#123;x: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> item&#125;)<br>        <br>        <span class="hljs-keyword">return</span> new_data<br>                <br>    <span class="hljs-comment"># 训练</span><br>    P, Q = &#123;&#125;, &#123;&#125;<br>    <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> train:<br>        P[user] = np.random.random(K)<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items:<br>        Q[item] = np.random.random(K)<br>            <br>    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> trange(step):<br>        data = nSample(train, ratio)<br>        <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> data:<br>            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[user]:<br>                eui = data[user][item] - (P[user] * Q[item]).<span class="hljs-built_in">sum</span>()<br>                P[user] += lr * (Q[item] * eui - lmbda * P[user])<br>                Q[item] += lr * (P[user] * eui - lmbda * Q[item])<br>        lr *= <span class="hljs-number">0.9</span> <span class="hljs-comment"># 调整学习率</span><br>        <br>    <span class="hljs-comment"># 获取接口函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">GetRecommendation</span>(<span class="hljs-params">user</span>):<br>        seen_items = <span class="hljs-built_in">set</span>(train[user])<br>        recs = &#123;&#125;<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items:<br>            <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> seen_items:<br>                recs[item] = (P[user] * Q[item]).<span class="hljs-built_in">sum</span>()<br>        recs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">sorted</span>(recs.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>))[:N]<br>        <span class="hljs-keyword">return</span> recs<br>    <br>    <span class="hljs-keyword">return</span> GetRecommendation<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># LFM实验(运行时间较长)</span><br>M, N = <span class="hljs-number">8</span>, <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>]:<br>    exp = Experiment(M, N, ratio=r)<br>    exp.run()<br></code></pre></td></tr></table></figure><p>在LFM中,重要的参数有4个:</p><ul><li>隐特征的个数：$F$</li><li>学习率：$\alpha$</li><li>正则化参数：$\lambda$</li><li>负样本&#x2F;正样本比例：$ratio$</li></ul><p>在该实验中,$ratio&gt;10$以后,精度就比较稳定.同时随着负样本数目的增加,覆盖率不断降低,流行度升高.说明该参数控制了推荐算法挖掘长尾的能力.</p><p>当数据非常稀疏时,$LFM$性能会明显下降,甚至低于$UserCF$和$ItemCF$.</p><h4 id="2-5-2-基于LFM实际系统的例子"><a href="#2-5-2-基于LFM实际系统的例子" class="headerlink" title="2.5.2 基于LFM实际系统的例子"></a>2.5.2 基于LFM实际系统的例子</h4><p>Yahoo的研究人员以CTR为优化目标,利用LFM来预测用户是否会单机一个链接,他们将用户历史上对首页上链接的行为记录在作为训练集.</p><p>在新闻推荐中,冷启动问题问题很明显.每天都有大量新新闻,在很短的时间内得到和失去人们的关注.因此实时性很重要.</p><p>但LFM模型在实际使用中很难实现实时的推荐.经典的LFM模型每次训练都需要扫描所有的用户行为记录才能计算出用户隐类向量$p_u$和物品引隐类向量$q_i$,此外,训练也比较费时.实际中往往是一天训练一次.因此不能实时地调整推荐结果来满足用户最近的行为.</p><p>他们的解决方案分为两部分<br>$$<br>r_{ui}&#x3D;x_u^T\cdot y_i+p_u^T\cdot q_i<br>$$<br>用户向量$x_u$可以根据历史行为获得,每天只需计算一次.$y_i$根据物品的内容属性直接生成.$p_u$和$q_i$是根据实时拿到的用户最近几小时的行为训练LFM获得的.</p><p>因此对于一个新加入的物品$i$,可以通过$x_u^T\cdot y_i$估计用户$u$对物品$i$的兴趣,经过几小时后,就能通过$p_u^T\cdot q_i$得到更准确的预测值.</p><h4 id="2-5-3-LFM和基于邻域的方法的比较"><a href="#2-5-3-LFM和基于邻域的方法的比较" class="headerlink" title="2.5.3 LFM和基于邻域的方法的比较"></a>2.5.3 LFM和基于邻域的方法的比较</h4><ul><li><strong>理论基础</strong>    LFM有比较好的理论基础,通过优化设定目标建立最优模型;基于邻域的方法是一种基于统计的方法</li><li><strong>离线计算的空间复杂度</strong>    假设有$M$个用户和$N$个物品.在计算相关表的过程中可能会获得一张比较稠密的临时相关表,例如用户相关表$O(M \ast M)$或物品相关表$O(N\ast N)$,但$LFM$在建模过程中如果是$F$个隐类,那么需要的存储空间是$O(F\ast (M+N))$.</li><li><strong>离线计算的时间复杂度</strong>    假设有$M$个用户,$N$个物品,$K$条用户对物品的行为记录.$UserCF$的时间复杂度为$O(N\ast (K&#x2F;N)^2)$,$ItemCF$的时间复杂度为$O(M\ast (K&#x2F;M)^2)$.$LFM$如果用$F$个隐类,迭代$S$次,复杂度为$O(K\ast F\ast S)$一般情况下$LFM$会稍高一点,但无本质区别.</li><li><strong>在线实时推荐</strong>    $UserCF$和$ltemCF$在线服务算法需要将相关表缓存在内存中，然后可以在线进行实时的预测.$LFM$不能进行在线实时推荐，也就是说，当用户有了新的行为后，他的推荐列表不会发生变化。</li><li><strong>推荐解释</strong>    $ltemCF$算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果。但LFM无法提供这样的解释，它计算出的隐类虽然在语义上确实代表了一类兴趣和物品，却很难用自然语言描述并生成解释展现给用户。</li></ul><h2 id="2-6-基于图的模型"><a href="#2-6-基于图的模型" class="headerlink" title="2.6 基于图的模型"></a>2.6 基于图的模型</h2><p>用户行为很容易用二分图表示,因此很多图的算法都可以用到推荐系统中</p><h4 id="2-6-1-用户行为数据的二分图表示"><a href="#2-6-1-用户行为数据的二分图表示" class="headerlink" title="2.6.1 用户行为数据的二分图表示"></a>2.6.1 用户行为数据的二分图表示</h4><p>令$G(V,E)$表示用户物品二分图,$V&#x3D;V_U\cup V_I$由用户顶点集合$V_U$和物品顶点集合$V_I$组成.对于数据集每一个二元组$(u,i)$,图中都有一套对应的边$e(v_u,v_i)$.图2-18是一个用户物品二分图模型,圆形节点代表用户,方形代表物品,边表示用户对物品的行为.</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220319183843603.png" alt="图2-18 用户物品二分图模型" style="zoom:45%;" /></div><h4 id="2-6-2-基于图的推荐算法"><a href="#2-6-2-基于图的推荐算法" class="headerlink" title="2.6.2 基于图的推荐算法"></a>2.6.2 基于图的推荐算法</h4><p>此时给用户$u$推荐物品的任务可以转化为度量用户顶点$v_u$和与$v_u$没有边直接相连的物品节点在图上的相关性,相关性越高的物品在推荐列表中的权重就越高.</p><p>图中<strong>顶点的相关性主要取决于下面三个因素:</strong></p><ul><li>两个顶点之间的路径数；</li><li>两个顶点之间路径的长度；</li><li>两个顶点之间的路径经过的顶点.</li></ul><p>而<strong>相关性高的一对顶点一般具有如下特征：</strong></p><ul><li>两个顶点之间有很多路径相连；</li><li>连接两个顶点之间的路径长度都比较短；</li><li>连接两个顶点之间的路径不会经过出度比较大的顶点.</li></ul><p>以下图为例;</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220319191551064.png" alt="图2-19 基于图的推荐算法示例" style="zoom: 50%;" /></div><p>用户$A$和物品$c、e$没有边相连，但是用户$A$和物品$c$有一条长度为$3$的路径相连，用户$A$和物品$e$有两条长度为$3$的路径相连。那么，顶点$A$与$e$之间的相关性要高于顶点$A$与$c$，因而物品$e$在用户$A$的推荐列表中应该排在物品$c$之前，因为顶点$A$与$e$之间有两条路径——$(A,b,C,e)$和$(A,d,D,e)$.其中,$(A,b,C,e)$路径经过的顶点的出度为$(3,2,2,2)$而$(A,d,D,e)$路径经过的顶点的出度为$(3,2,3,2)$.因此,$(A,d,D,e)$经过了一个出度比较大的顶点$D$，所以$(A,d,D,e)$对顶点$A$与$e$之间相关性的贡献要小于$(A,b,C,e)$.</p><p>基于上面三个主要因素,有很多计算图中顶点之间相关性的方法,如基于随机游走的$PersonalRank$算法.</p><h5 id="PersonalRank-算法"><a href="#PersonalRank-算法" class="headerlink" title="$PersonalRank$算法"></a>$PersonalRank$算法</h5><p>可以看看<a href="https://zhuanlan.zhihu.com/p/341989445">这篇笔记</a></p><p>假设要给用户$u$进行个性化推荐，可以从用户$u$对应的节点$v_u$开始,在用户物品二分图上进行随机游走。游走到任何一个节点时，首先按照概率$\alpha$决定是继续游走，还是停止这次游走并从$v_u$节点开始重新游走。如果决定继续游走，那么就从当前节点指向的节点中按照<strong>均匀分布</strong>随机选择一个节点作为游走下次经过的节点。这样，经过很多次随机游走后，每个物品节点被访问到的概率会<strong>收敛到一个数</strong>。最终的推荐列表中物品的权重就是物品节点的访问概率。<br>如果将上面的描述表示成公式，可以得到如下公式：</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220319200225631.png" alt="" style="zoom: 50%;" /></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">PersonalRank</span>(<span class="hljs-params">train, alpha, N</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :params: train, 训练数据</span><br><span class="hljs-string">    :params: alpha, 继续随机游走的概率</span><br><span class="hljs-string">    :params: N, 推荐TopN物品的个数</span><br><span class="hljs-string">    :return: GetRecommendation, 获取推荐结果的接口</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span> <br>    <br>    <span class="hljs-comment"># 构建索引</span><br>    items = []<br>    <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> train:<br>        items.extend(train[user])<br>    id2item = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(items))<br>    users = &#123;u: i <span class="hljs-keyword">for</span> i, u <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train.keys())&#125;<br>    items = &#123;u: i+<span class="hljs-built_in">len</span>(users) <span class="hljs-keyword">for</span> i, u <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(id2item)&#125;<br>    <br>    <span class="hljs-comment"># 计算转移矩阵（注意！！！要按照出度进行归一化）</span><br>    item_user = &#123;&#125;<br>    <span class="hljs-keyword">for</span> user <span class="hljs-keyword">in</span> train:<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> train[user]:<br>            <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> item_user:<br>                item_user[item] = []<br>            item_user[item].append(user)<br>            <br>    data, row, col = [], [], []<br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> train:<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> train[u]:<br>            data.append(<span class="hljs-number">1</span> / <span class="hljs-built_in">len</span>(train[u]))<br>            row.append(users[u])<br>            col.append(items[v])<br>    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> item_user:<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> item_user[u]:<br>            data.append(<span class="hljs-number">1</span> / <span class="hljs-built_in">len</span>(item_user[u]))<br>            row.append(items[u])<br>            col.append(users[v])<br>            <br>    M = csc_matrix((data, (row, col)), shape=(<span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(data)))<br>    <br>    <span class="hljs-comment"># 获取接口函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">GetRecommendation</span>(<span class="hljs-params">user</span>):<br>        seen_items = <span class="hljs-built_in">set</span>(train[user])<br>        <span class="hljs-comment"># 解矩阵方程 r = (1-a)r0 + a(M.T)r</span><br>        r0 = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(data)<br>        r0[users[user]] = <span class="hljs-number">1</span><br>        r0 = csc_matrix(r0)<br>        r = (<span class="hljs-number">1</span> - alpha) * linalg.inv(eye(<span class="hljs-built_in">len</span>(data)) - alpha * M.T) * r0<br>        r = r.T.toarray()[<span class="hljs-number">0</span>][<span class="hljs-built_in">len</span>(users):]<br>        idx = np.argsort(-r)[:N]<br>        recs = [(id2item[ii], r[ii]) <span class="hljs-keyword">for</span> ii <span class="hljs-keyword">in</span> idx]<br>        <span class="hljs-keyword">return</span> recs<br>    <br>    <span class="hljs-keyword">return</span> GetRecommendation<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># PersonalRank实验</span><br>M, N, alpha = <span class="hljs-number">8</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.8</span><br>exp = Experiment(M, N, alpha)<br>exp.run()<br></code></pre></td></tr></table></figure><p>$PersonalRank$算法有较好的理论解释,但算法复杂度很高.因为在为每个用户进行推荐时，都需要在整个用户物品二分图上进行迭代，直到整个图上的每个顶点的$PR$值收敛。这一过程的时间复杂度非常高，不仅无法在线提供实时推荐，甚至离线生成推荐结果也很耗时。</p><p>解决方案一:减少迭代次数，在收敛之前就停止。这样会影响最终的精度，但一般来说影响不会特别大。</p><p>解决方案二:将$PersonalRank$转化为矩阵的形式。令$M$为用户物品二分图的转移概率矩阵，即：<br>$$<br>M(v,v’)&#x3D;\frac{1}{|out(v)|}<br>$$<br>迭代公式可转化为:<br>$$<br>r&#x3D;(1-\alpha)r_0+\alpha M^T&#x3D;(1-\alpha)(1-\alpha M^T)^{-1}r_0<br>$$<br>这时只需利用稀疏矩阵快速求逆的办法来计算$(1-\alpha M^T)^{-1}$即可.</p>]]></content>
    
    
    
    <tags>
      
      <tag>读书笔记</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推荐系统实践（项亮）第一章笔记</title>
    <link href="/2022/03/16/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
    <url>/2022/03/16/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%EF%BC%88%E9%A1%B9%E4%BA%AE%EF%BC%89%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="第一章-好的推荐系统"><a href="#第一章-好的推荐系统" class="headerlink" title="第一章 好的推荐系统"></a>第一章 好的推荐系统</h1><h2 id="1-1-什么是推荐系统"><a href="#1-1-什么是推荐系统" class="headerlink" title="1.1 什么是推荐系统"></a>1.1 什么是推荐系统</h2><p>以看电影为例：</p><p>社会化推荐：找朋友问，发布动态等待热心人评论。既让好友给自己推荐物品。</p><p>基于内容的推荐：输入喜欢的演员名，发现没看过的就去观看。这种方式是寻找和自己之前看过的在内容上相似的电影。推荐系统可以将上述过程自动化，通过分析用户的喜欢的导演、演员等，然后给用户推荐相关电影。</p><p>基于协同过滤的推荐：看看别人都在看什么，找一部广受好评的进行观看。即找到和自己历史兴趣相似的一群用户，这样的结果可能比宽泛的热门排行榜更能符合自己的兴趣。</p><p>因此，推荐算法的本质是通过一定的方式将用户和物品联系起来，而不同的推荐系统利用了不同的方式。如图1-2展示了三种方式。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220316150237617.png" alt="图1-2 推荐系统常用的3种联系用户和物品的方式" style="zoom:50%;" /></div><h2 id="1-2-个性化推荐系统的应用"><a href="#1-2-个性化推荐系统的应用" class="headerlink" title="1.2 个性化推荐系统的应用"></a>1.2 个性化推荐系统的应用</h2><p>与搜索引擎不同，个性化推荐系统组要依赖用户的行为数据，因此一般是作为一个应用存在于不同的网站中。其通过大量分析用户日志进而提供给用户不同的个性化页面展示，来提高网站的CTR和CVR。</p><p>广泛应用RS的领域包括电子商务，阅读，音乐，个性化邮件和广告等。但总的来说，<strong>几乎所有的推荐系统都是由前台的展示页面、后台的日志系统、以及推荐算法系统3部分构成。</strong>下面围绕这三个方面对不同的个性化推荐系统应用进行介绍。</p><hr><p>　　CVR (Click Value Rate): 转化率，衡量CPA广告效果的指标<br>　　CTR (Click Through Rate): 点击率<br>　　CPC (Cost Per Click): 按点击计费<br>　　CPA (Cost Per Action): 按成果数计费<br>　　CPM (Cost Per Mille): 按千次展现计费<br>　　PV (Page View): 流量<br>　　PV单价: 每PV的收入，衡量页面流量变现能力的指标<br>　　ADPV (Advertisement Page View): 载有广告的pageview流量<br>　　ADimp (ADimpression): 单个广告的展示次数<br>　　RPS (Revenue Per Search): 每搜索产生的收入，衡量搜索结果变现能力指标<br>　　ROI：投资回报率(ROI)是指通过投资而应返回的价值，它涵盖了企业的获利目标。利润和投入的经营所必备的财产相关，因为管理人员必须通过投资和现有财产获得利润。又称会计收益率、投资利润率。</p><hr><h3 id="1-2-1-电子商务"><a href="#1-2-1-电子商务" class="headerlink" title="1.2.1 电子商务"></a>1.2.1 电子商务</h3><p>以亚马逊的推荐系统为例，最主要的应用有<strong>个性化商品推荐列表</strong>和<strong>相关商品的推荐列表</strong>。</p><p>个性化推荐列表（两种）：</p><ul><li>基于物品的推荐算法，给用户推荐那些和他们之前喜欢的物品相似的物品</li><li>按照用户在脸书上的好友关系，给用户推荐他们好友在亚马逊上喜欢的物品</li></ul><p>相关推荐列表（两种）：</p><ul><li>买了这个商品的用户也经常购买的其他商品</li><li>浏览过这个商品的用户经常购买的其他用品</li></ul><p>两者使用了不同用户行为计算物品的相关性。此外。会让你选择是否同时购买，他会把这几件商品“打包”，同时可能会提供折扣。这种销售手段是推荐算法最重要的应用。</p><h3 id="1-2-2-电影和视频网站"><a href="#1-2-2-电影和视频网站" class="headerlink" title="1.2.2 电影和视频网站"></a>1.2.2 电影和视频网站</h3><p>…</p><h3 id="1-2-8-个性化广告"><a href="#1-2-8-个性化广告" class="headerlink" title="1.2.8 个性化广告"></a>1.2.8 个性化广告</h3><p>个性化广告投放目前已经成为了一们独立的学科——计算广告学——该学科和推荐系统在很多基础理论和方法上是相通的，比如他们的目的都是联系用户和物品，只是在个性化广告中，物品就是广告。</p><p>个性化广告投放和狭义个性化推荐的区别是，个性化推荐着重于帮助用户找到可能令他们感兴趣的物品，而广告推荐着重于帮助广告找到可能对它们感兴趣的用户，即一个是以用户为核心，而另一个以广告为核心。目前的个性化广告投放技术主要分为3种。</p><ul><li><strong>上下文广告</strong>$\quad$通过分析用户正在浏览的网页内容，投放和网页内容相关的广告。</li><li><strong>搜索广告</strong>$\quad$通过分析用户在当前会话中的搜索记录，判断用户的搜索目的，投放和用户目的相关的广告。</li><li><strong>个性化展示广告</strong>$\quad$我们经常在很多网站看到大量展示广告（就是那些大的横幅图片），它们是根据用户的兴趣，对不同用户投放不同的展示广告。雅虎是这方面研究的代表。</li></ul><h2 id="1-3-推荐系统评测"><a href="#1-3-推荐系统评测" class="headerlink" title="1.3 推荐系统评测"></a>1.3 推荐系统评测</h2><p>什么才是好的推荐系统？这是推荐系统评测需要解决的首要问题。一个完整的推荐系统一般存在3个参与方（如图1-22所示）：<strong>用户、物品提供者和提供推荐系统的网站</strong>。</p><p>以图书推荐为例，首先，推荐系统需要满足用户的需求，给用户推荐那些令他们感兴趣的图书。其次，推荐系统要让各出版社的书都能够被推荐给对其感兴趣的用户，而不是只推荐几个大型出版社的书。最后，好的推荐系统设计，能够让推荐系统本身收集到高质量的用户反馈，不断完善推荐的质量，增加用户和网站的交互，提高网站的收入。</p><p>因此在评测一个推荐算法时，需要同时考虑三方的利益，一个好的推荐系统是能够令三方共赢的系统。</p><p>需要注意的是，<strong>做出准确预测的推荐系统并不意味着好的推荐系统</strong>。假如一个图书推荐系统预测一个用户将来会购买《C++Primer中文版》这本书，而用户后来确实均买了，那么这就被看做一次准确的预测。<strong>预测准确度</strong>是推荐系统领域的重要指标（没有之一）。这个指标的好处是，它可以<strong>比较容易地通过离线方式计算出来</strong>，<strong>从而方便研究人员快速评价和选择不同的推荐算法。</strong>但是，很多研究表明，准确的预测并不代表好的推荐。比如说，该用户早就准备买《C++Primer中文版》了，无论是否给他推荐，他都准备购买，那么这个推荐结果显然是不好的，因为它并未使用户购买更多的书，而仅仅是方便用户购买一本他本来就准备买的书。</p><p>那么，对于用户来说，他会觉得这个推荐结果很不新颖，不能令他惊喜。同时，对于《C++Primer中文版》的出版社来说，这个推荐也没能增加这本书的潜在购买人数。所以，这是一个看上去很好，但其实却很失败的推荐。举一个更极端的例子，某推测系统预测明天太阳将从东方升起，虽然预测准确率是100%，却是一种没有意义的预测。</p><p>好的推荐系统不仅仅能够准确预测用户的行为，还能帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。同时，推荐系统还要能够<strong>帮助商家</strong>将那些被埋没在长尾中的好商品介绍给可能会对它们感兴趣的用户。</p><p>为了全面评测推荐系统对三方利益的影响，本章将从不同角度出发，提出不同的指标。这些指标包括<strong>准确度、覆盖度、新颖度、惊喜度、信任度、透明度等</strong>。这些指标中，有些可以离线计算，有些只有在线才能计算，有些只能通过用户问卷获得。下面各节将会依次介绍这些指标的出发点、含义，以及一些指标的计算方法。</p><h3 id="1-3-1-推荐系统实验方法"><a href="#1-3-1-推荐系统实验方法" class="headerlink" title="1.3.1 推荐系统实验方法"></a>1.3.1 推荐系统实验方法</h3><p>三种评测推荐效果的实验方法：</p><ul><li>离线实验（offline experiment）</li><li>用户调查（user study）</li><li>在线实验（online experiment）</li></ul><h4 id="1-离线实验"><a href="#1-离线实验" class="headerlink" title="1. 离线实验"></a>1. 离线实验</h4><p>   离线实验一般由以下几个步骤构成：</p><p>   (1) 通过日志系统获得用户行为数据，并按照一定格式生成一个表针的数据集</p><p>   (2) 将数据按规则分为训练集和测试集</p><p>   (3) 在训练集上训练用户兴趣模型，在测试集上进行预测</p><p>   (4) 通过事先定义的离线指标评测算法在测试集上的预测结果</p><p>​        从上面的步骤可以看到，推荐系统的<strong>离线实验都是在数据集上完成的</strong>，也就是说它<strong>不需要一个实际的系统来供它实验</strong>，而只要有一个从实际系统日志中提取的数据集即可。这种实验方法的<strong>好处是不需要真实用户参与</strong>，<strong>可以直接快速地计算出来，从而方便、快速地测试大量不同的算法。</strong>主要缺点是无法获得很多商业上关注的指标，如点击率、转化率等，而找到和商业指标非常相关的离线指标也是很困难的事情。表1-2简单总结了离线实验的优缺点点。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220316150237617.png" alt="图1-2 推荐系统常用的3种联系用户和物品的方式" style="zoom:50%;" /></div><table><thead><tr><th align="center">优点</th><th align="center">缺点</th></tr></thead><tbody><tr><td align="center">不需要对实际系统的控制权</td><td align="center">无法计算商业上关心的指标</td></tr><tr><td align="center">不需要用户参与实验</td><td align="center">离线实验的指标和商业指标存在差距</td></tr><tr><td align="center">速度快</td><td align="center"></td></tr></tbody></table><h4 id="2-用户调查"><a href="#2-用户调查" class="headerlink" title="2. 用户调查"></a>2. 用户调查</h4><p>​        离线实验的指标和实际的商业指标存在差距，如预测准确率和用户满意度。因此<strong>准确评测算法需要相对比较真实的环境</strong>，最好的方法就是将算法直接上线测试，但可能会降低用户满意度，因此在上线前需要做一次成为用户调查的测试。</p><p>​        用户调查需要有一些<strong>真实用户</strong>，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，我们需要观察和记录他们的行为，并让他们回答一些问题。最后，我们需要通过分析他们的行为和答案了解测试系统的性能。离线时没有办法评测的与用户主观感受有关的指标都可以通过用户调查获得。</p><p>​        <strong>缺点：</strong>成本很高，很难进行大规模的用户调查，人数较少的用户调查，往往没有统计意义。此外，需要尽量保证测试用户的分布和真实用户的<strong>分布相同</strong>，比如男女各半，以及年龄、活跃度的分布都和真实用户分布尽量相同。用户调查要尽量保证是<strong>双盲实验</strong>，不要让实验人员和用户事先知道测试目标，以免测试受主观成分的影响。（在很多时候设计双盲实验非常困难，因而在测试环境下收集的测试指标可能在真实环境下无法重现。）<br>       <strong>优点：</strong>可以获得体现用户主观感受的指标，在线实验风险很低，出现错误后很容易弥补。</p><h4 id="3-在线实验"><a href="#3-在线实验" class="headerlink" title="3. 在线实验"></a>3. 在线实验</h4><p>​        完成离线实验和用户调查后,可将推荐系统上线做<strong>AB测试</strong>,将其和旧的算法进行比较</p><p>​        <strong>AB测试</strong>是一种<strong>在线评测算法</strong>的实验方法,通过一定规则将用户随机分为几组,不同组采用不同算法,进而统计各自的评测指标来比较不同算法.</p><p>​        **优点:**可公平获得不同算法实际在显示的性能指标</p><p>​        **缺点:**周期过长,需要长期实验才能得到可靠的结果.因此一般只会用AB测试测试在离线实验和用户调查中表现很好的算法.同时考虑这样一种情况,对于一个大型网站,其前端和后端由不同团队控制,可能进行一个后台推荐算法AB测试时网页团队在做推荐页面的界面AB测试.因此,切分流量是AB测试中的关键，不同的层以及控制这些层的团队需要从一个统一的地方获得自己AB测试的流量，而不同层之间的流量应该是正交的。</p><p>​        图1-23是一个简单的AB测试系统,用户进入网站后，流量分配系统决定用户是否需要被进行AB测试，如果需要的话，流量分配系统会给用户打上在测试中属于什么分组的标签。然后用户浏览网页，而用户在浏览网页时的行为都会被通过日志系统发回后台的日志数据库。此时，如果用户有测试分组的标签，那么该标签也会被发回后台数据库。</p><p>​        在后台，实验人员的工作首先是配置流量分配系统，决定满足什么条件的用户参加什么样的测试。其次，实验人员需要统计日志数据库中的数据，通过评测系统生成不同分组用户的实验报告，并比较和评测实验结果。</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220314215941918.png" alt="图1-23AB测试系统" style="zoom: 67%;" /></div><p>​        一般来说,一个新的推荐系统最终上线,需要完成上面所说的三个实验:</p><ul><li>通过<strong>离线实验</strong>证明它在很多<strong>离线指标</strong>上优于现有的算法</li><li>通过<strong>用户调查</strong>确定它的<strong>用户满意度</strong>不低于现有的算法</li><li>通过在线的<strong>AB测试</strong>确定它在我们<strong>关心的指标</strong>上优于现有的算法</li></ul><h3 id="1-3-2-评测指标"><a href="#1-3-2-评测指标" class="headerlink" title="1.3.2 评测指标"></a>1.3.2 评测指标</h3><h4 id="1-用户满意度"><a href="#1-用户满意度" class="headerlink" title="1.用户满意度"></a>1.用户满意度</h4><p>​        无法离线计算,只能通过用户调查或在线实验获得.</p><p>​        用户调查获得用户满意度一般是通过调查问卷的形式,问卷的设计不是简单的满意&#x2F;不满意,应考虑到各个感受.</p><p>​        在线系统中,用户满意度主要通过对用户行为的统计得到.例如电商网站中,购买率可以度量用户的满意度.或者是设计用户反馈页面手机用户满意度,如bilibili下的不感兴趣.</p><p>​        更一般的情况,可以用CTR,用户停留时间和转化率等指标度量用户的满意度.</p><h4 id="2-预测准确度"><a href="#2-预测准确度" class="headerlink" title="2. 预测准确度"></a>2. 预测准确度</h4><p>​        度量一个推荐系统或者推荐算法<strong>预测用户行为的能力</strong>,是<strong>最重要的系统离线评测指标</strong>,大多数paper讨论的也是这个指标(可通过离线实验计算,方便研究员研究推荐算法)</p><p>​        计算该指标需要一个离线的数据集,通过在训练集上建立用户的行为和兴趣模型预测用户在测试集上行为.</p><p>​        离线的推荐算法有不同的研究方向,下面为不同的研究方向对应的预测准确度指标.</p><ul><li><strong>评分预测</strong></li></ul><p>学习用户的历史评分,进而预测其在将来看到一个他没有评过分的物品时,会给这个物品评多少分.</p><p>评分预测的预测准确度一般通过均方根误差 (RMSE) 和平均绝对误差 (MAE) 计算.对于测试集$T$中的一个用户$u$和物品$i$,令$r_{ui}$代表用户对物品的实际评分,令$\hat{r}_{ui}$为预测评分.</p><ul><li>RMSE定义为：</li></ul><p>$$<br>RMSE&#x3D;\sqrt{\frac{\sum_{u,i\in T}(r_{ui}-\hat{r_{ui})^2}}{|T|}}<br>$$</p><ul><li>MAE采用绝对值计算预测误差，其定义为：</li></ul><p>$$<br>MAE&#x3D;\frac{\sum_{u,i\in T}|r_{ui}-\hat r_{ui}|}{|T|}<br>$$</p><p>​        假设用一个列表<code>records</code>存放用户评分数据,令<code>records[i]=[u,i,rui,pui]</code>,后两者分别为实际和预测得分,则下面的代码实现了RMSE和MAE的计算过程</p><figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">RMSE</span>(<span class="hljs-params">records</span>):<br><span class="hljs-keyword">return</span> math.sqrt(<span class="hljs-built_in">sum</span>([(rui-pui)*(rui-pui) <span class="hljs-keyword">for</span> u,i,rui,pui <span class="hljs-keyword">in</span> records])/<span class="hljs-built_in">float</span>(<span class="hljs-built_in">len</span>(records)))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MAE</span>(<span class="hljs-params">records</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">abs</span>(rui-pui) <span class="hljs-keyword">for</span> u,i,rui,pui <span class="hljs-keyword">in</span> records])/<span class="hljs-built_in">float</span>(<span class="hljs-built_in">len</span>(records))<br></code></pre></td></tr></table></figure><p>​        关于这两个指标的优缺点:</p><p>​        RMSE加大了对预测不准的用户物品评分的惩罚,因而对系统的评测更加苛刻</p><p>小tips:如果评分系统是基于整数建立的(即用户给的评分都是整数),那么对预测结果取整会降低MAE的误差.</p><ul><li>TopN推荐</li></ul><p>​        网站在提供推荐服务时,一般是给用户一个<strong>个性化的推荐列表</strong>,这种推荐叫做TopN推荐.其预测准确率一般通过准确率(precision)&#x2F;召回率(recall)度量.<br>​        令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表,$T(u)$是用户在测试集上的行为列表,那么推荐结果的召回率定义为:<br>$$<br>Recall&#x3D;\frac{\sum_{u\in U}|R(U)\cap T(U)|}{\sum_{u\in U}T(U)}<br>$$<br>​        推荐结果的准确率定义为:<br>$$<br>Precision&#x3D;\frac{\sum_{u\in U}|R(U)\cap T(U|}{\sum_{u\in U}R(U)}<br>$$<br>​        代码实现:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">PrecisionRecall</span>(<span class="hljs-params">test,N</span>):<br>    hit = <span class="hljs-number">0</span><br>    n_recall=<span class="hljs-number">0</span><br>    n_precision=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> user,items <span class="hljs-keyword">in</span> test.items():<br>        rank=Recommend(user,N) //rank:预测的列表-长度为N(n_precision)<br>        hit+=<span class="hljs-built_in">len</span>(rank&amp;items)   //items:实际的列表-T(U)-长度为<span class="hljs-built_in">len</span>(items)(n_recall)<br>        n_recall+=<span class="hljs-built_in">len</span>(items)<br>        n_precision+=N<br>    <span class="hljs-keyword">return</span> [hit/(<span class="hljs-number">1.0</span> * n_recall),hit/(<span class="hljs-number">1.0</span> * n_precision)<br></code></pre></td></tr></table></figure><p>​        为了全面评测TopN推荐的准确率和召回率,一般会选取不同的推荐长度N,计算出一组准确率&#x2F;召回率,然后画出准确率&#x2F;召回率曲线(precision&#x2F;recall curve).</p><ul><li>评分预测 VS TopN推荐</li></ul><p>研究人员一般都将精力集中在优化评分预测的RMSE上,但后来有人指出,电影推荐的目的是找到用户可能感兴趣的电影,而非预测看了电影后给出什么样的评分.例如,也许有一部分电影用户看了后会给出高分,但看的几率很小.因此,预测用户是否会看比看了之后给什么评分更加重要.因此,之后主要讨论TopN推荐.</p><h4 id="3-覆盖率"><a href="#3-覆盖率" class="headerlink" title="3. 覆盖率"></a>3. 覆盖率</h4><p>覆盖率(coverage)描述一个推荐系统对物品长尾的发掘能力.一般定义为推荐系统能够推荐出来的物品占总物品的比例,假设用户集合为$U$,RS给每个用户推荐一个长度为N的物品列表$R(u)$,则覆盖率可通过以下公式计算:<br>$$<br>Coverage&#x3D;\frac{|\cup_{u\in U}R(U)|}{|I|}<br>$$<br>因此,这是一个内容提供商会关注的指标.值为100%时,每个物品都至少被推荐给一个用户.好的推荐系统要有较高的用户满意度和较高的覆盖率.</p><p>需要注意的是,上面的定义是粗略的,因为100%覆盖率的RS可以有无数的物品流行度分布.因此可通过研究<strong>物品在推荐列表中出现次数的分布</strong>描述推荐系统挖掘长尾的能力,若该分布比较平,就说明覆盖率较高,陡峭则低.</p><p>在信息论和经济学中,还有两个指标可定义覆盖率:</p><ul><li>信息熵</li></ul><p>$$<br>H&#x3D;-\sum_{i&#x3D;1}^np(i)log\ p(i)<br>$$</p><p>$p(i)$指物品$i$的流行度占所有物品流行度的比.</p><ul><li>基尼系数(Gini Index)</li></ul><p>$$<br>G&#x3D;\frac{1}{n-1}\sum_{j&#x3D;1}^n(2j-n-1)p(i_j)<br>$$</p><p>$p(i_j)$是物品流行度$p()$按照<strong>从小到大</strong>排序的物品列表中第$j$个物品.</p><div align=center><img src="https://hicgoal-img.oss-cn-beijing.aliyuncs.com/img/image-20220315154509482.png" alt="基尼系数示例图" style="zoom:50%;" /></div><p>基尼系数的计算原理：</p><p>将物品按照热门程度从低到高排列，则右图中的黑色曲线表示最不热门的$x %$物品的总流行度占系统的比例$y%$。则这条曲线定在$y&#x3D;x$曲线之下。</p><p>基尼系数的形象定义就是$\frac{SA}{SA+SB}$,看以看出属于区间$[0,1]$,如果系统流行度很平均,$SA$会很小,基尼系数也很小,反之亦成立.</p><p>考虑这样一个问题,进入热门排行榜的物品都是热门商品,这些商品因为被放在榜中展示有了更多的曝光机会,因此会更加热门,这就是马太效应.</p><p>推荐系统的初衷是消除马太效应,使各种商品都能被展示给对其感兴趣的某一类人群.但一些推荐算法(如协同过滤)是具有马太效应的.</p><p>评测RS是否有马太效应的方法就是使用基尼系数.若$G_1$是从初始用户行为中计算得出的物品流行度的基尼系数,$G_2$是从推荐列表中计算出的物品流行度的基尼系数.如果$G_2&gt;G_1$,就说明推荐算法有马太效应.</p><h4 id="4-多样性"><a href="#4-多样性" class="headerlink" title="4. 多样性"></a>4. 多样性</h4><p>用户兴趣可能是广泛的,一个人可能喜欢《猫和老鼠》的同时也喜欢纪录片,因此推荐系统应该能够覆盖用户不同的兴趣领域.</p><p>此外，尽管用户的兴趣在较长的时间跨度是不一样的,但具体到用户访问的<strong>某一刻</strong>,其兴趣往往是单一的,如果推荐的某个兴趣点恰恰不是用户此时的兴趣点,推荐列表就不会让用户满意.因此推荐结果要具有多样性.</p><p>多样性定义了推荐列表中物品两两之间的不相似性.因此,多样性和相似性是对应的.假设$s(i,j)\in [0,1]$代表物品$i$和$j$之间的相似度,那么<strong>用户$u$的推荐列表$R(u)$多样性</strong>定义为:<br>$$<br>Diversity(R(u))&#x3D;1-\frac{\sum_{i,j\in R(u),i\neq j}s(i,j)}{\frac{1}{2}|R(u)|(|R(u)|-1)}<br>$$<br>推荐系统的整体多样性可定义为所有用户列表多样性的平均值:<br>$$<br>Diversity&#x3D;\frac{1}{|U|}\sum_{u\in U}Diversity(R(u))<br>$$<br>显然,不同的物品相似度$s(i,j)$可定义不同的多样性.e.g.,若用内容相似度描述$s(i,j)$,则可得到内容多样性函数;用协同过滤的的相似度函数描述$s(i,j)$,得到协同过滤的多样性函数.</p><p>推荐系统的多样性达到什么程度叫好?</p><p>e.g. 假设某用户喜欢A,B,且80%时间看A,20%时间看B.则最理想的推荐(以推荐10个为例)为推荐8个A,2个B</p><h4 id="5-新颖性"><a href="#5-新颖性" class="headerlink" title="5. 新颖性"></a>5. 新颖性</h4><p>新颖的推荐是指给用户推荐他们以前没有听说过的物品.<br>实现新颖性一个简单的办法是,把那些用户之前在网站中交互过的物品从推荐列表中过滤掉<br>新颖性可以利用推荐结果的平均流行度来度量.推荐结果中的平均热门程度越低推荐结果就可能有比较高的新颖性.这种评测比较粗略,因为不同用户不知道的东西不同,可能一个人偏爱冷门物品，此时对大多数人来说高新颖性的推荐列表反而对该用户低新颖性。因此准确的统计需要做用户调查<br>困难的是如何在不牺牲精度的情况下提高多样性和新颖性.</p><h4 id="6-惊喜度"><a href="#6-惊喜度" class="headerlink" title="6. 惊喜度"></a>6. 惊喜度</h4><p>基本意思:如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高<br>而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果(推荐的电影主演和用户喜欢的主演一样,只是该用户之前没看过,这可以看作新颖性.但用户一旦了解了主演就不会特别奇怪,因此一般不视作惊喜度)。<br>目前并没有什么公认的惊喜度指标定义方式，这里只给出一种定性的度量方式:定义惊喜度需要首先定义推荐结果和用户历史上喜欢的物品的相似度，其次需要定义用户对推荐结果的满意度。前面也曾提到，用户满意度只能通过问卷调查或者在线实验获得，而推荐结果和用户历史上喜欢的物品相似度一般可以用内容相似度定义。<br>e.g. 如果获得了一个用户观看电影的历史，得到这些电影的演员和导演集合A，然后给用户推荐一个不属于集合A的导演和演员创作的电影，而用户表示非常满意，这样就实现了一个惊喜度很高的推荐。<br>因此<strong>提高推荐惊喜度需要提高推荐结果的用户满意度，同时降低推荐结果和用户历史兴趣的相似度。</strong></p><h4 id="7-信任度"><a href="#7-信任度" class="headerlink" title="7. 信任度"></a>7. 信任度</h4><p>考虑两个推荐系统,它们的推荐结果相同，但用户却可能产生不同的反应，这就是因为用户对他们有不同的信任度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电于商务推荐系统中，让用户对推荐结果产生信任非常重要.<br>该指标的度量只能通过问卷调查的方式.<br>提高推荐系统的信任度主要有两种方法。</p><ul><li>增加推荐系统的透明度（transpareney)，主要办法是提供推荐解释。只有让用户了解推荐系统的运行机制，让用户认同推荐系统的运行机制，才会提高用户对推荐系统的信任度。- 考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。这是因为用户对他们的好友一般都比较信任.</li></ul><h4 id="8-实时性"><a href="#8-实时性" class="headerlink" title="8. 实时性"></a>8. 实时性</h4><p>某些物品(新闻,微博)有很强的时效性,因此需要在其具有时效性时就将它们推荐给用户.RS的实时性包括两个方面:</p><ul><li>需要实时地更新推荐列表来满足用户新的行为变化。比如，当一个用户购买了iPhone，如果推荐系统能够立即给他推荐相关配件，那么肯定比第二天再给用户推荐相关配件更有价值。很多推荐系统都会在离线状态每天计算一次用户推荐列表，然后于在线期间将推荐列表展示给用户。这种设计显然是无法满足实时性的。与用户行为相应的实时性，评测方法:推荐列表的变化速率。如果推荐列表在用户有行为后变化不大，或者没有变化，说明推荐系统的实时性不高。</li><li>需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。评测方法:可以利用用户推荐列表中有多大比例的物品是当天新加的来评测。</li></ul><h4 id="9-健壮性"><a href="#9-健壮性" class="headerlink" title="9. 健壮性"></a>9. 健壮性</h4><p>健壮性（即robust鲁棒性）衡量推荐系统抗击作弊的能力。e.g.行为注入攻击:亚马逊有一种推荐叫做“购买商品A的用户也经常购买的其他商品”。它的主要计算方法是统计购买商品A的用户购买其他商品的次数。可以攻击这个算法，让<strong>自己的商品</strong>在这个推荐列表中获得比较高的排名，比如可以注册很多账号，用这些账号同时购买A和自己的商品。还有一种攻击主要针对评分系统雇用一批人给自己的商品非常高的评分，而评分行为是推荐系统依赖的重要用户行为。<br>算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法<br>给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。<br>在实际系统中，提高系统的健壮性，除了选择健壮性高的算法，还有以下方法。</p><ul><li>设计推荐系统时尽量使用代价比较高的用户行为。比如，如果有用户购买行为和用户浏<br>览行为，那么主要应该使用用户购买行为，因为购买需要付费，所以攻击购买行为的代<br>价远远大于攻击浏览行为。</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h4 id="10-商业目标"><a href="#10-商业目标" class="headerlink" title="10.商业目标"></a>10.商业目标</h4><p>不同公司会根据自己的盈利模式设计不同的商业目标,例如电商网站的目标可能是销售额,基于展示&#x2F;点击广告盈利的网站其商业目标可能是广告&#x2F;点击展示总数.<br>因此,设计推荐系统需要考虑最终的商业目标,网站使用推荐系统的目的除了满足用户发现内容的需求,也需要利用推荐系统加快实现商业上的目标.</p><h4 id="11-总结"><a href="#11-总结" class="headerlink" title="11.总结"></a>11.总结</h4><p>上面提到的指标有些可以离线计算,有的只能在线获得.如何<strong>优化离线指标来提高在线指标</strong>是推荐系统研究的重要问题.<br>$\circ$指得到的统计不准确</p><p>发</p><table><thead><tr><th align="center"></th><th align="center">离线实验</th><th align="center">文件调查</th><th align="center">在线实验</th></tr></thead><tbody><tr><td align="center">用户满意度</td><td align="center">$\times$</td><td align="center">$\checkmark$</td><td align="center">$\circ$</td></tr><tr><td align="center">预测准确度</td><td align="center">$\checkmark$</td><td align="center">$\checkmark$</td><td align="center">$\times$</td></tr><tr><td align="center">覆盖率</td><td align="center">$\checkmark$</td><td align="center">$\checkmark$</td><td align="center">$\checkmark$</td></tr><tr><td align="center">多样性</td><td align="center">$\circ$</td><td align="center">$\checkmark$</td><td align="center">$\circ$</td></tr><tr><td align="center">新颖性</td><td align="center">$\circ$</td><td align="center">$\checkmark$</td><td align="center">$\circ$</td></tr><tr><td align="center">惊喜度</td><td align="center">$\times$</td><td align="center">$\checkmark$</td><td align="center">$\times$</td></tr></tbody></table><p>对于可以离线优化的指标，书中认为应该是在给定覆盖率、多样性、新颖性等限制条件下，尽量优化预测准确度。因此离线实验的优化目标可以表示为：<br>$$<br>最大化预测准确度\<br>使得\quad 覆盖率&gt;A\<br>\qquad \quad多样性&gt;B\<br>\qquad \quad新颖性&gt;C<br>$$</p><h2 id="1-3-3-评测维度"><a href="#1-3-3-评测维度" class="headerlink" title="1.3.3 评测维度"></a>1.3.3 评测维度</h2><p>评测系统除了上述指标还要考虑评测维度.增加评测维度能让我们知道一个算法在什么情况下性能最好,为融合不同推荐算法取得最好的整体性能带来参考.<br>一般评测维度分为以下三种:</p><ul><li><strong>用户维度</strong>主要包括用户的人口统计学信息、活跃度以及是不是新用户等。</li><li><strong>物品维度</strong>包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。</li><li><strong>时间维度</strong>包括季节，是工作日还是周末，是白天还是晚上等。<br>如果能够在推荐系统评测报告中包含不同维度下的系统评测指标，就能帮我们全面地了解推荐系统性能，找到一个看上去比较弱的算法的优势，发现一个看上去比较强的算法的缺点。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>读书笔记</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/02/25/hello-world/"/>
    <url>/2022/02/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
